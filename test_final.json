[
  {
    "paper_id": "",
    "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
    "authors": [
      "Jon Saad-Falcon",
      "Omar Khattab",
      "Keshav Santhanam",
      "Radu Florian",
      "Martin Franz",
      "Salim Roukos",
      "Avirup Sil",
      "Md Arafat Sultan",
      "Christopher Potts"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Saad-Falcon et al., 2025",
    "abstract": "Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains and achieves substantially lower latency than standard reranking methods.",
    "bit": "Conventional information retrieval methods rely on large labeled datasets for fine-tuning, which can be expensive or impractical to obtain. Moreover, even when such datasets are available, their effectiveness rapidly decreases in new domains due to distributional shifts.",
    "flip": "Instead, a multi-step approach for synthetic data generation leverages a high-end LLM to create a small set of queries and then employs a lower-cost LLM to scale up the synthetic query pool. This allows for fine-tuning multiple rerankers, which are subsequently distilled into a single lightweight model that maintains strong performance and lower latency across diverse domains.",
    "spark": "Scalable synthetic queries for retrieval",
    "chain_of_reasoning": "I recognized that obtaining large labeled datasets for IR tasks is often expensive or infeasible, which prompted the question: how could I circumvent that reliance while preserving robust performance? I observed that generating a small seed set of queries with a high-end generative model was promising, but I wondered how to scale this effectively and cheaply. I tested a strategy where a more cost-efficient model would expand the seed set into a large synthetic corpus, and I validated performance by measuring how well the resulting rerankers handled domain shifts. How would I ensure that these synthetic queries captured the nuances of the target domain, and what experimental design would safeguard against overfitting to contrived patterns? I tuned the rerankers on the expanded synthetic data, then asked if I could unify their strengths without incurring additional latency. After refining a distillation phase, I realized that integrating all rerankers' insights into one lightweight model was precise and fast, which was a turning point in achieving broad efficacy. I measured zero-shot accuracy in several long-tail domains and observed meaningful improvements, confirming that my approach successfully mitigated distributional shifts. To strengthen the analysis, I performed further validation by comparing retrieval metrics across diverse datasets, ensuring consistent reliability. I remain interested in optimizing data generation parameters, such as balancing diversity and relevance in synthetic queries, to further enhance performance. Reflecting on these findings, I see a cohesive pipeline: seed generation with a powerful model, bulk expansion with a cheaper model, fine-tuning, and final distillation, all of which address the drawbacks of labeled data scarcity.",
    "url": "https://arxiv.org/abs/2303.00807",
    "pdf_url": "https://arxiv.org/pdf/2303.00807.pdf"
  },
  {
    "paper_id": "",
    "title": "LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
    "authors": [
      "Chi Han",
      "Qifan Wang",
      "Hao Peng",
      "Wenhan Xiong",
      "Yu Chen",
      "Heng Ji",
      "Sinong Wang"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Han et al., 2025",
    "abstract": "Today's large language models (LLMs) typically train on short text segments (e.g., <4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encoding scientific articles, code repositories, or long dialogues. Through theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis further reveals that commonly used techniques like truncating the attention window or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7x decoding speed up and 7.5x memory saving over the original model. Our codes are released at \\url{https://github.com/Glaciohound/LM-Infinite}.",
    "bit": "Large language models typically train on short text segments, such as those under 4K tokens, because the sequential complexity of Transformer architectures grows quadratically with length. This approach significantly degrades their performance on inputs longer than the training context, limiting their effectiveness on tasks that demand extensive contextual understanding.",
    "flip": "An approach called LM-Infinite was introduced to allow large language models to handle extremely long sequences, even up to 200 million tokens. By preserving perplexity through flexible transformations of the input and attention mechanisms, it enables zero-shot length generalization without additional parameter updates and provides substantial efficiency gains.",
    "spark": "Infinite context for large LLMs",
    "chain_of_reasoning": "I first asked myself why models break down so severely when facing extremely long sequences, which made me scrutinize the quadratic scaling of attention. I observed that conventional training segments refuse to accommodate contexts far beyond 4K tokens, so I investigated a proposed transformation that preserves perplexity without brute force extensions. I questioned whether parameter updates were essential, yet experimentation with a flexible input pipeline and attention mechanism revealed no additional tuning required. Reflecting on the empirical results, I found that the model handled text lengths up to 200 million tokens while retaining perplexity and outperforming baseline approaches on tasks like Passkey Retrieval and Qasper in zero-shot settings. I noted substantial benefits in both decoding time and memory usage: it ran 2.7 times faster and consumed 7.5 times less memory. This prompted me to explore how the newly introduced strategy generalizes across varied context lengths, using a range of text inputs to confirm its robustness. Seeing consistent performance gains, I posed further questions about extending these methods to specialized domains, such as code or multilingual corpora, to validate reliability. The critical turning point came when I grasped that the emergent stability across vastly larger contexts hinges on carefully preserving positional encodings. Ultimately, I recommend follow-up experiments refining the input transformations for specialized data distributions, ensuring the final model scales advantageously without compromising accuracy.",
    "url": "https://arxiv.org/abs/2308.16137",
    "pdf_url": "https://arxiv.org/pdf/2308.16137.pdf"
  },
  {
    "paper_id": "",
    "title": "Large Language Models Can Learn Temporal Reasoning",
    "authors": [
      "Siheng Xiong",
      "Ali Payani",
      "Ramana Kompella",
      "Faramarz Fekri"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Xiong et al., 2025",
    "abstract": "While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal concepts and intricate temporal logic. In this paper, we propose TG-LLM, a novel framework towards language-based TR. Instead of reasoning over the original context, we adopt a latent representation, temporal graph (TG) that enhances the learning of TR. A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task. We confirmed in experiments that the capability of TG translation learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation. We observed that those strategies, which maintain a balance between usefulness and diversity, bring more reliable CoTs and final results than the vanilla CoT distillation.",
    "bit": "Large language models typically rely on a direct reading of text to handle temporal reasoning, causing them to struggle with complex time-dependent logic. Their linear approach often fails when the temporal concepts interact in intricate ways, exposing limitations in accuracy and interpretability.",
    "flip": "An approach was introduced that uses a latent temporal graph rather than the original text to capture rich temporal relationships. It employs a synthetic dataset for text-to-graph translation and leverages chain-of-thought bootstrapping plus graph data augmentation to produce more reliable reasoning steps.",
    "spark": "Graph-based approach to temporal reasoning",
    "chain_of_reasoning": "I recognized that an ordinary direct reading approach is prone to confusion when subtle temporal concepts overlap in multiple ways. I then asked myself: would a latent temporal graph representation disentangle these dependencies more effectively? To examine this, I assembled a synthetic dataset that pairs text with carefully structured graph labels, providing minimal but precise guidance for text-to-graph translation. I wondered: does this synthetic resource accurately reflect real-world complexities and avoid potential overfitting? Upon obtaining promising model performance on external temporal reasoning tasks, I saw that focusing on graph-based representations reduced the known linear limitation. I also introduced chain-of-thought bootstrapping, questioning how to ensure each reasoning step remains consistent and robust. Graph data augmentation offered a balanced means to diversify training while preserving logical coherence, raising the question of whether additional validation on broader benchmarks might confirm reliability. Observing consistent improvements in accuracy and interpretability reassured me of a turning point in handling intricate time-dependent logic. Finally, the synergy between latent temporal graphs, chain-of-thought, and a carefully controlled data generation pipeline strengthened my confidence in this methodology. This iterative pursuit, guided by systematic experimentation and varied validations, provided a solid foundation for advancing language-based temporal reasoning.",
    "url": "https://arxiv.org/abs/2401.06853",
    "pdf_url": "https://arxiv.org/pdf/2401.06853.pdf"
  },
  {
    "paper_id": "",
    "title": "R-Tuning: Instructing Large Language Models to Say `I Don't Know'",
    "authors": [
      "Hanning Zhang",
      "Shizhe Diao",
      "Yong Lin",
      "Yi R. Fung",
      "Qing Lian",
      "Xingyao Wang",
      "Yangyi Chen",
      "Heng Ji",
      "Tong Zhang"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Zhang et al., 2025",
    "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning.",
    "bit": "Conventional instruction tuning methods force language models to generate answers even when queries exceed their known scope. This creates a major limitation where the model often produces hallucinations and fails to signal uncertainty.",
    "flip": "A refusal-aware technique is introduced, where the model is trained using data that highlights knowledge gaps between pre-training and instruction tuning. By providing explicit refusal examples for out-of-scope queries, the model learns to refrain from responding when it lacks the necessary information.",
    "spark": "Refusal-based approach for unknown queries",
    "chain_of_reasoning": "I first asked myself how forced generation was causing spurious answers, and I observed that out-of-knowledge questions triggered guesswork. In response, I reasoned that embedding explicit refusal data in the training set could teach the model to withhold a response when lacking certain information. I wondered how to construct such data correctly, so I derived it from a systematic comparison of pre-training domain coverage versus instruction coverage. Next, I tested the approach on specialized and out-of-domain tasks and found that the new technique effectively suppressed incorrect elaborations. A pivotal question emerged: would the model preserve general answer accuracy while gaining this refusal skill? My experiments showed that the model not only maintained its in-scope performance, but also generalized an uncertainty-awareness across different tasks. I probed the results further by analyzing calibration metrics, which improved consistently and indicated strengthened ability to gauge knowledge boundaries. One critical question regarding validation was whether the approach yielded robust refusals without sacrificing coverage, and the data analyses confirmed a stable balance. Finally, I reflected that the learned refusal behavior served as a meta-competence, supporting my hypothesis that the model could internalize uncertainty as an actionable skill.",
    "url": "https://arxiv.org/abs/2311.09677",
    "pdf_url": "https://arxiv.org/pdf/2311.09677.pdf"
  },
  {
    "paper_id": "",
    "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
    "authors": [
      "Oded Ovadia",
      "Menachem Brief",
      "Moshik Mishaeli",
      "Oren Elisha"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Ovadia et al., 2025",
    "abstract": "Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.",
    "bit": "In conventional practice, large language models rely heavily on their pre-trained weights to handle knowledge across diverse domains. However, this built-in knowledge is still constrained by the scope and biases of the original training data, making it difficult to seamlessly incorporate updated or external facts.",
    "flip": "An approach promotes retrieval-augmented generation, which dynamically references external knowledge sources rather than exclusively updating model parameters. By leveraging these external repositories during inference, models can more effectively expand or refine their factual coverage without exhaustive fine-tuning.",
    "spark": "Retrieval-based generation surpasses unsupervised fine-tuning",
    "chain_of_reasoning": "When I recognized that large language models store knowledge in their internal parameters, I questioned whether fine-tuning alone could reliably add new facts or refine old ones. I curated diverse datasets covering different knowledge-intensive fields and implemented unsupervised fine-tuning, yet I noticed only modest improvements. I asked myself whether providing multiple variations of the same fact in training could enhance knowledge updates, so I introduced repeated rephrasings into the fine-tuning corpus. Preliminary experiments suggested incremental gains, but truly novel information continued to challenge the model\u2019s internal representation. I then proposed dynamically referencing external repositories that the model could incorporate during inference without overhauling its core parameters. I tested retrieval-augmented generation using curated external knowledge sources, analyzing how consistently the approach integrated new information. To validate this method, I measured performance across tasks demanding newly acquired content and observed clear gains over unsupervised fine-tuning. My direct comparison showed retrieval-augmented generation provided more reliable factual grounding, especially when confronted with knowledge unseen in the original training data. I realized that connecting external references with the model\u2019s existing structure was the pivotal step that overcame the deficiency in updating internal weights. However, I remain concerned about how to maintain up-to-date external sources efficiently, prompting further study of memory system design and version control.",
    "url": "https://arxiv.org/abs/2312.05934",
    "pdf_url": "https://arxiv.org/pdf/2312.05934.pdf"
  },
  {
    "paper_id": "",
    "title": "Prompts have evil twins",
    "authors": [
      "Rimon Melamed",
      "Lucas H. McCabe",
      "Tanay Wakhare",
      "Yejin Kim",
      "H. Howie Huang",
      "Enric Boix-Adsera"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Melamed et al., 2025",
    "abstract": "We discover that many natural-language prompts can be replaced by corresponding prompts that are unintelligible to humans but that provably elicit similar behavior in language models. We call these prompts \"evil twins\" because they are obfuscated and uninterpretable (evil), but at the same time mimic the functionality of the original natural-language prompts (twins). Remarkably, evil twins transfer between models. We find these prompts by solving a maximum-likelihood problem which has applications of independent interest.",
    "bit": "Traditionally, language models are guided by precisely crafted natural-language prompts to control their outputs. However, these prompts must remain interpretable to humans, which can limit scale, automation, and flexibility.",
    "flip": "In contrast, an approach was proposed that replaces natural-language prompts with unintelligible prompts while preserving similar model behaviors. By solving a maximum-likelihood formulation, these 'evil twin' prompts are systematically derived to mimic the functionality of readable prompts across different language models.",
    "spark": "Obfuscated prompts replicating original tasks",
    "chain_of_reasoning": "I began by asking: \u201cDo prompts truly need to remain readable to humans to effectively guide language models?\u201d and I recognized that human-interpretability can constrain automation. I explored a maximum-likelihood procedure that systematically altered token choices to generate prompts that resemble the original instructions functionally but remain unintelligible. Next, I implemented this technique across multiple language models to verify whether such obfuscated prompts would consistently transfer, and I noted stable performance in producing similar outputs. I then asked: \u201cCan these hidden sequences replicate the token-level distributions of the original prompts?\u201d and comprehensive tests confirmed that the outputs align closely in likelihood and content. I observed a turning point when I realized that, despite the obfuscation, these substitutes still encoded transferable instructions across distinct architectures. My experiments also included measuring equivalence through perplexity and functional alignment, which reinforced the idea that these seemingly random substitutes preserve core task representations. I asked further: \u201cHow should I validate robustness to adversarial alterations?\u201d and preliminary tests showed that small perturbations do not destructively degrade prompt efficacy. Finally, I plan deeper evaluations with additional domains to confirm that domain shifts do not undermine the obfuscation method, and I remain confident this maximum-likelihood approach can transform how we craft flexible, automated language prompts.",
    "url": "https://arxiv.org/abs/2311.07064",
    "pdf_url": "https://arxiv.org/pdf/2311.07064.pdf"
  },
  {
    "paper_id": "",
    "title": "AstroCLIP: A Cross-Modal Foundation Model for Galaxies",
    "authors": [
      "Liam Parker",
      "Francois Lanusse",
      "Siavash Golkar",
      "Leopoldo Sarra",
      "Miles Cranmer",
      "Alberto Bietti",
      "Michael Eickenberg",
      "Geraud Krawezik",
      "Michael McCabe",
      "Ruben Ohana",
      "Mariel Pettee",
      "Bruno Regaldo-Saint Blancard",
      "Tiberiu Tesileanu",
      "Kyunghyun Cho",
      "Shirley Ho"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Parker et al., 2025",
    "abstract": "We present AstroCLIP, a single, versatile model that can embed both galaxy images and spectra into a shared, physically meaningful latent space. These embeddings can then be used - without any model fine-tuning - for a variety of downstream tasks including (1) accurate in-modality and cross-modality semantic similarity search, (2) photometric redshift estimation, (3) galaxy property estimation from both images and spectra, and (4) morphology classification. Our approach to implementing AstroCLIP consists of two parts. First, we embed galaxy images and spectra separately by pretraining separate transformer-based image and spectrum encoders in self-supervised settings. We then align the encoders using a contrastive loss. We apply our method to spectra from the Dark Energy Spectroscopic Instrument and images from its corresponding Legacy Imaging Survey. Overall, we find remarkable performance on all downstream tasks, even relative to supervised baselines. For example, for a task like photometric redshift prediction, we find similar performance to a specifically-trained ResNet18, and for additional tasks like physical property estimation (stellar mass, age, metallicity, and sSFR), we beat this supervised baseline by 19\\% in terms of $R^2$. We also compare our results to a state-of-the-art self-supervised single-modal model for galaxy images, and find that our approach outperforms this benchmark by roughly a factor of two on photometric redshift estimation and physical property prediction in terms of $R^2$, while remaining roughly in-line in terms of morphology classification. Ultimately, our approach represents the first cross-modal self-supervised model for galaxies, and the first self-supervised transformer-based architectures for galaxy images and spectra.",
    "bit": "Previous approaches typically relied on separate supervised models for galaxy images and spectra, each narrowly tuned to specific tasks. This segmentation limited their ability to leverage cross-modal information and often required extensive fine-tuning for new objectives.",
    "flip": "Instead, a unified strategy was introduced that uses transformer-based encoders trained in a self-supervised setting for both images and spectra, then aligns these encoders using a contrastive loss. This shared latent space enables versatile cross-modal applications, including photometric redshift estimation and property prediction, without additional fine-tuning.",
    "spark": "Unified cross-modal self-supervised galaxy insights",
    "chain_of_reasoning": "Observing that separately trained models restricted flexible cross-modal learning and required substantial re-tuning, I asked myself how best to embed images and spectra together while preserving key astrophysical features. I decided to train two transformer-based encoders, each in a self-supervised setting, because they can capture complex spatial and spectral correlations without manually labeled data. I then used a contrastive loss to align these encoders, bringing corresponding galaxy images and spectra closer in a shared latent space while retaining physical interpretability. To verify that this alignment supported diverse tasks, I tested photometric redshift estimation and property prediction, measuring metrics like R\u00b2 and comparing results with a carefully trained ResNet18 baseline. I observed that this unified approach met or surpassed dedicated supervised methods, prompting me to expand experiments into morphology classification to ensure robust cross-modal performance. I asked: does this method remain accurate for more subtle morphological distinctions, and can it handle broad data ranges without extra fine-tuning? The results showed that the shared embedding stayed highly predictive of redshifts and physical properties, while morphology classification maintained strong accuracy. Reflecting on the data, I realized that a single framework could match or exceed separate specialized models, suggesting a significant advance in flexibility. I also queried whether class imbalance might degrade the embedding's reliability, so I conducted stratified checks and found stable performance across rarer subsets. Ultimately, I concluded that this cohesive transformer-based strategy, combining self-supervised pretraining with cross-modal alignment, is a powerful new tool for comprehensive galaxy analyses.",
    "url": "https://arxiv.org/abs/2310.03024",
    "pdf_url": "https://arxiv.org/pdf/2310.03024.pdf"
  },
  {
    "paper_id": "",
    "title": "Stay on topic with Classifier-Free Guidance",
    "authors": [
      "Guillaume Sanchez",
      "Honglu Fan",
      "Alexander Spangher",
      "Elad Levi",
      "Pawan Sasanka Ammanamanchi",
      "Stella Biderman"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Sanchez et al., 2025",
    "abstract": "Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline.",
    "bit": "Conventional text-based generative models typically rely on standard decoding methods that treat the model\u2019s output solely as a direct reflection of its learned parameters. While effective for many tasks, this approach often demands extensive fine-tuning or large parameter counts to maintain robust performance, reducing its flexibility during inference.",
    "flip": "An idea was introduced to apply a technique, originally used in text-to-image generation, called Classifier-Free Guidance as an inference-time method in language modeling. By injecting a secondary guidance term during decoding, the model can better adhere to prompts, improve coherence, and enhance performance across diverse tasks.",
    "spark": "Broad inference-time improvements with CFG",
    "chain_of_reasoning": "I began by questioning whether I could escape the typical reliance on standard decoding methods that demand large parameter counts by simply adjusting the inference strategy. Would the approach applied in text-to-image generation, where a secondary guiding signal is integrated, be adaptable to language modeling tasks including Q&A, reasoning, code generation, and translation? In a series of experiments on annotated benchmarks, I injected the guidance factor at decode-time, systematically modulating it to gauge improvements in prompt adherence without large-scale fine-tuning. I observed performance gains comparable to doubling model capacity, including surpassing a much larger reference model on a well-known language comprehension task, which compelled me to ask: do I see this effect across different architectures, including GPT-2, Pythia, and LLaMA-based systems? Revisiting the results, I noted consistent improvements on tasks such as reasoning and code generation, prompting me to integrate chain-of-thought and self-consistency alongside this guidance approach for compounded benefits. How do I confirm that the method was not introducing unintended biases or focusing too narrowly on certain prompts? I performed multiple human and quantitative evaluations, revealing that my approach substantially boosted coherence, accuracy, and faithfulness, demonstrating a 75% preference over a baseline approach. A crucial turning point arose when I realized that adjusting guidance magnitude yielded robust synergy for various domain-specific tasks without sacrificing creativity. These findings drove me to refine the guidance term further, ensuring stable results even as I scaled the model or changed domain contexts. Moving forward, I plan to expand these experiments with additional metrics and improved validation protocols, seeking to solidify the methodology\u2019s generalization across more intricate tasks.",
    "url": "https://arxiv.org/abs/2306.17806",
    "pdf_url": "https://arxiv.org/pdf/2306.17806.pdf"
  },
  {
    "paper_id": "",
    "title": "On the nature of disks at high redshift seen by JWST/CEERS with contrastive learning and cosmological simulations",
    "authors": [
      "J. Vega-Ferrero",
      "M. Huertas-Company",
      "L. Costantin",
      "P. G. P\u00e9rez-Gonz\u00e1lez",
      "R. Sarmiento",
      "J. S. Kartaltepe",
      "A. Pillepich",
      "M. B. Bagley",
      "S. L. Finkelstein",
      "E. J. McGrath",
      "J. H. Knapen",
      "P. Arrabal Haro",
      "E. F. Bell",
      "F. Buitrago",
      "A. Calabr\u00f2",
      "A. Dekel",
      "M. Dickinson",
      "H. Dom\u00ednguez S\u00e1nchez",
      "D. Elbaz",
      "H. C. Ferguson",
      "M. Giavalisco",
      "B. W. Holwerda",
      "D. D. Kocesvski",
      "A. M. Koekemoer",
      "V. Pandya",
      "C. Papovich",
      "N. Pirzkal",
      "J. Primack",
      "L. Y. Aaron Yung"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Vega-Ferrero et al., 2025",
    "abstract": "Visual inspections of the first optical rest-frame images from JWST have indicated a surprisingly high fraction of disk galaxies at high redshifts. Here, we alternatively apply self-supervised machine learning to explore the morphological diversity at $z \\geq 3$. Our proposed data-driven representation scheme of galaxy morphologies, calibrated on mock images from the TNG50 simulation, is shown to be robust to noise and to correlate well with the physical properties of the simulated galaxies, including their 3D structure. We apply the method simultaneously to F200W and F356W galaxy images of a mass-complete sample ($M_*/M_\\odot>10^9$) at $ 3 \\leq z \\leq 6$ from the first JWST/NIRCam CEERS data release. We find that the simulated and observed galaxies do not exactly populate the same manifold in the representation space from contrastive learning. We also find that half the galaxies classified as disks -- either CNN-based or visually -- populate a similar region of the representation space as TNG50 galaxies with low stellar specific angular momentum and non-oblate structure. Although our data-driven study does not allow us to firmly conclude on the true nature of these galaxies, it suggests that the disk fraction at $z \\geq 3$ remains uncertain and possibly overestimated by traditional supervised classifications. Deeper imaging and spectroscopic follow-ups as well as comparisons with other simulations will help to unambiguously determine the true nature of these galaxies, and establish more robust constraints on the emergence of disks at very high redshift.",
    "bit": "Traditionally, morphological classifications of high-redshift galaxies rely on supervised models or visual inspections, assuming disk structures remain clearly identifiable even with limited image quality. However, noise and biases often lead to uncertain or inflated estimates of the disk fraction at these early epochs.",
    "flip": "A data-driven approach uses self-supervised machine learning, leveraging simulated galaxy images as calibration data, to robustly characterize morphologies despite noise. By learning a representation through contrastive methods, it captures subtle morphological signatures that better correlate with the underlying 3D structure of galaxies.",
    "spark": "Self-supervised galaxy morphology classification",
    "chain_of_reasoning": "I first questioned how well conventional supervised models could handle noisy high-redshift data. Realizing that morphological classification results might be inflated, I tested a self-supervised approach with mock images from the TNG50 simulation to calibrate a representation robust to noise. Then I asked whether contrastive learning could reveal hidden morphological features correlated with physical properties. Observing that the final representation space did not perfectly align the CEERS data with the simulation made me wonder if some visually identified disks might be non-oblate in 3D. I specifically examined a mass-complete sample with M_* \u2265 10^9 M_\u2299 at z \u2265 3 in two NIRCam bands to ensure consistent morphological metrics. The results suggested that half of the so-called disk galaxies occupied a region resembling limited angular momentum objects in TNG50, indicating a possible overestimate of the disk fraction. Did my data analysis incorporate variance from noise levels correctly, and how might deeper imaging improve confidence in these measurements? Reassessing each contrastive embedding step affirmed that spectroscopic follow-ups and comparisons with other simulations are crucial for validation. An important turning point occurred when I realized that combining data-driven embeddings with direct observational analysis might be the key to resolving morphological ambiguities. I will refine the pipeline with more calibration checks to safeguard against overestimating the emergence of early disks.",
    "url": "https://arxiv.org/abs/2302.07277",
    "pdf_url": "https://arxiv.org/pdf/2302.07277.pdf"
  },
  {
    "paper_id": "",
    "title": "Unlocking Emergent Modularity in Large Language Models",
    "authors": [
      "Zihan Qiu",
      "Zeyu Huang",
      "Jie Fu"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Qiu et al., 2025",
    "abstract": "Modular Neural Networks (MNNs) demonstrate various advantages over monolithic models. Existing MNNs are generally $\\textit{explicit}$: their modular architectures are pre-defined, with individual modules expected to implement distinct functions. Recent works reveal that there exists $\\textit{implicit}$ modularity in standard pre-trained transformers, namely $\\textit{Emergent Modularity}$. They indicate that such modular structures spontaneously exhibit during the early pre-training phase. Despite the benefits of modularity, most Language Models (LMs) are still treated as monolithic models in the pre-train and fine-tune paradigm, with their emergent modularity locked and underutilized. In this work, focusing on unlocking the emergent modularity in LMs, we showcase that standard LMs could be fine-tuned as their Mixture-of-Expert (MoEs) counterparts without introducing any extra parameters. Such MoEs are derived from emergent modularity and are referred to as Emergent MoEs (EMoE). Our experiments demonstrate that fine-tuning EMoE effectively improves downstream in-domain and out-of-domain generalization compared with vanilla fine-tuning. Our analysis and ablation studies further illustrate that it is robust to various configurations and can scale up to Large Language Models (i.e., Llama2-7B and Llama-30B). Code is available at https://github.com/qiuzh20/EMoE.",
    "bit": "Most large language models are treated as monolithic systems during the pre-train and fine-tune process, leaving any naturally emerging modular structure underexploited. This conventional approach overlooks the potential gains in performance and generalization from leveraging these spontaneously formed specialized regions within the network.",
    "flip": "Instead, a method was introduced that recognizes and activates these latent modules to fine-tune standard language models as mixture-of-experts without adding any extra parameters. By selectively routing information through the spontaneously formed modules, the approach achieves stronger in-domain and out-of-domain performance compared to the traditional monolithic fine-tuning paradigm.",
    "spark": "Activating emergent modularity for better generalization",
    "chain_of_reasoning": "I first wondered whether standard pre-trained transformers might already contain latent substructures that could be harnessed. I then asked: how do I systematically reveal these specialized regions without altering the model\u2019s overall design? My response was to identify the naturally formed partitions of the network, which had been shown to emerge during early pre-training, and treat them as separate experts. Observing the improved downstream performance when I selectively routed data through these implicitly formed groups, I recognized that no new parameters were needed for a mixture-of-experts. I questioned whether this approach would hold for larger models and varied data distributions, so I conducted in-domain and out-of-domain evaluations, finding consistent gains. I further asked if different module selection strategies would alter generalization, and saw that multiple configurations yielded robust improvements. These experimental results validated the idea that activating existing modularity within the network surpassed traditional single-expert fine-tuning. My next inquiry concerned the best way to confirm the stability of these derived experts, so I ran ablation studies reflecting minimal performance drops across configurations. In that moment, I realized that harnessing spontaneously organized modules could be the key to notably stronger adaptation without expanding the model size.",
    "url": "https://arxiv.org/abs/2310.10908",
    "pdf_url": "https://arxiv.org/pdf/2310.10908.pdf"
  },
  {
    "paper_id": "",
    "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
    "authors": [
      "Mikayel Samvelyan",
      "Sharath Chandra Raparthy",
      "Andrei Lupu",
      "Eric Hambro",
      "Aram H. Markosyan",
      "Manish Bhatt",
      "Yuning Mao",
      "Minqi Jiang",
      "Jack Parker-Holder",
      "Jakob Foerster",
      "Tim Rockt\u00e4schel",
      "Roberta Raileanu"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Samvelyan et al., 2025",
    "abstract": "As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.",
    "bit": "Traditional methods for crafting adversarial prompts often concentrate on narrowly defined domains or rely heavily on manual curation. This approach limits the breadth of attack scenarios and hinders discovery of diverse vulnerabilities in large language models.",
    "flip": "A novel technique reframes adversarial prompt generation as a quality-diversity problem and leverages open-ended search to systematically uncover a wide array of effective attacks. This strategy produces both high attack success rates and transferable prompts, enabling models to be fine-tuned more robustly without harming their overall performance.",
    "spark": "Open-ended quest for diverse exploits",
    "chain_of_reasoning": "I began by recognizing that existing approaches rely on narrow domains and extensive manual curation, limiting exploration of diverse model weaknesses. I asked myself how a broader and more systematic search strategy could reveal new vulnerabilities. When I tested an open-ended method that treats prompt generation as a quality-diversity problem, I saw significant expansions in attack scenarios. I confirmed this through black-box evaluations on models like Llama 2 and Llama 3, where I measured consistently high attack success rates. I then wondered if prompts derived from this method would transfer well to other tasks or architectures, and my experiments confirmed robust cross-model effectiveness. Next, I questioned whether adding these synthetic adversarial prompts might degrade general performance, but subsequent assessments showed no detrimental impacts on helpfulness. Reflecting on this, I found that fine-tuning with these prompts not only mitigated vulnerabilities but also preserved model capabilities. I re-evaluated the approach for question answering and cybersecurity use cases, discovering it supported broader applications effectively. At one point, I realized that guided exploration of prompt space could systematically expose hidden weaknesses and also spur more resilient model adaptations. Finally, I asked about refining data collection and validation protocols, envisioning repeated iterations of open-ended prompt search for continuous model improvement.",
    "url": "https://arxiv.org/abs/2402.16822",
    "pdf_url": "https://arxiv.org/pdf/2402.16822.pdf"
  },
  {
    "paper_id": "",
    "title": "QuRating: Selecting High-Quality Data for Training Language Models",
    "authors": [
      "Alexander Wettig",
      "Aatmik Gupta",
      "Saumya Malik",
      "Danqi Chen"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Wettig et al., 2025",
    "abstract": "Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that can capture human intuitions about data quality. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value - and find that LLMs are able to discern these qualities, especially when making pairwise judgments of texts. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity. When we sample using quality ratings as logits over documents, our models obtain lower perplexity and stronger in-context learning performance than baselines. Our best model is based on educational value and performs similarly to a model trained with uniform sampling for 50% more steps. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.",
    "bit": "Conventional data selection for language model pre-training relies on basic heuristics, limiting the capture of nuanced human-like judgments of data quality. This constraint can lead to models trained on less diverse or lower-quality data, ultimately restricting performance gains and generalization.",
    "flip": "Instead, an idea was introduced to learn scalar ratings from pairwise judgments, reflecting multiple dimensions of data quality such as style, expertise, factual accuracy, and educational value. By using these ratings to guide sampling and curricula, models can better balance diversity and quality, resulting in improved perplexity and in-context learning performance.",
    "spark": "Quality-based data selection for LLMs",
    "chain_of_reasoning": "I began by questioning whether a simple selection strategy overlooked subtleties in how humans judge textual quality, and I asked how to systematically encode nuanced perceptions of expertise and factual precision. I tested pairwise comparisons between texts to see if this approach offered deeper insight than raw heuristics, and I observed that large models were able to internally distinguish style and depth in surprising ways. Then I trained a specific model to transform these comparisons into scalar ratings for four dimensions and used these ratings to annotate an extensive corpus of training data. I wondered if strictly picking only the top-rated documents might shrink diversity too much, so I devised a sampling scheme that was guided by quality ratings but still spanned a wide range of content. During experiments on 30B tokens with 1.3B-parameter models, I found perplexity dropped considerably and in-context learning improved, indicating that diversity and quality ratings can both be preserved. I designed further tests to confirm whether consistency of ratings held across varied prompts, and I saw that the model retained a stable sense of each dimension. My main turning point arose when I noticed how consistently educational-value ratings drove faster improvement than uniform data coverage. This realization led me to implement a curriculum approach based on these scores, which further refined the model\u2019s gains. Finally, I analyzed potential biases in the rating outputs and planned additional experiments to quantify how each dimension might impact downstream tasks and ensure the method\u2019s validity.",
    "url": "https://arxiv.org/abs/2402.09739",
    "pdf_url": "https://arxiv.org/pdf/2402.09739.pdf"
  },
  {
    "paper_id": "",
    "title": "Measuring Sharpness in Grokking",
    "authors": [
      "Jack Miller",
      "Patrick Gleeson",
      "Charles O'Neill",
      "Thang Bui",
      "Noam Levi"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Miller et al., 2025",
    "abstract": "Neural networks sometimes exhibit grokking, a phenomenon where perfect or near-perfect performance is achieved on a validation set well after the same performance has been obtained on the corresponding training set. In this workshop paper, we introduce a robust technique for measuring grokking, based on fitting an appropriate functional form. We then use this to investigate the sharpness of transitions in training and validation accuracy under two settings. The first setting is the theoretical framework developed by Levi et al. (2023) where closed form expressions are readily accessible. The second setting is a two-layer MLP trained to predict the parity of bits, with grokking induced by the concealment strategy of Miller et al. (2023). We find that trends between relative grokking gap and grokking sharpness are similar in both settings when using absolute and relative measures of sharpness. Reflecting on this, we make progress toward explaining some trends and identify the need for further study to untangle the various mechanisms which influence the sharpness of grokking.",
    "bit": "Typically, training and validation performance are assumed to improve at similar rates, leaving little distinction in how accuracy transitions are measured. This overlooks a phenomenon where near-perfect validation accuracy can emerge abruptly well after training accuracy plateaus, creating a gap that is not well captured by existing methods.",
    "flip": "A new technique was introduced to measure grokking by fitting training and validation accuracy curves to an appropriate functional form. This framework provides a robust means to quantify the sharpness of accuracy transitions, enabling systematic comparisons across different experimental settings.",
    "spark": "Measuring grokking with functional fitting",
    "chain_of_reasoning": "I began by asking how to capture the sudden jump in validation accuracy that arose well after training accuracy stabilized. I developed a curve-fitting strategy to systematically compare how sharply accuracy rose in both the theoretical framework of Levi et al. (2023) and the parity-prediction MLP inspired by Miller et al. (2023). I noticed that in the theoretical setting, the closed-form expressions allowed us to isolate the fit parameters precisely. Then I checked if the same functional form could track the abruptness in the MLP data when grokking was induced by concealed patterns. To validate the method, I measured the sharpness of each transition repeatedly and confirmed consistent outcomes across runs. I next asked whether this approach might reveal hidden influences on timing and amplitude of the validation accuracy jump. Reflecting on the results, I recognized that separating the effect of relative gap from absolute growth clarified how grokking differs across tasks. At that point, I realized we could directly compare very distinct settings\u2014analytical and empirical\u2014using the same quantitative lens. Yet I remained curious about whether potential confounds, such as parameter initialization or data partitioning, would affect the derived measures of sharpness and warrant further experiments.",
    "url": "https://arxiv.org/abs/2402.08946",
    "pdf_url": "https://arxiv.org/pdf/2402.08946.pdf"
  },
  {
    "paper_id": "",
    "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
    "authors": [
      "Swagata Ashwani",
      "Kshiteesh Hegde",
      "Nishith Reddy Mannuru",
      "Mayank Jindal",
      "Dushyant Singh Sengar",
      "Krishna Chaitanya Rao Kathala",
      "Dishant Banga",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Ashwani et al., 2025",
    "abstract": "With the rise of Large Language Models(LLMs), it has become crucial to understand their capabilities and limitations in deciphering and explaining the complex web of causal relationships that language entails. Current methods use either explicit or implicit causal reasoning, yet there is a strong need for a unified approach combining both to tackle a wide array of causal relationships more effectively. This research proposes a novel architecture called Context Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to enhance causal reasoning and explainability. The proposed framework incorporates an explicit causal detection module with ConceptNet and counterfactual statements, as well as implicit causal detection through LLMs. Our framework goes one step further with a layer of counterfactual explanations to accentuate LLMs understanding of causality. The knowledge from ConceptNet enhances the performance of multiple causal reasoning tasks such as causal discovery, causal identification and counterfactual reasoning. The counterfactual sentences add explicit knowledge of the not caused by scenarios. By combining these powerful modules, our model aims to provide a deeper understanding of causal relationships, enabling enhanced interpretability. Evaluation of benchmark datasets shows improved performance across all metrics, such as accuracy, precision, recall, and F1 scores. We also introduce CausalNet, a new dataset accompanied by our code, to facilitate further research in this domain.",
    "bit": "Conventional causal reasoning approaches often focus on either explicit or implicit methods for detecting cause-effect relationships. This separation can lead to gaps in understanding the nuanced and interlinked nature of language-based causality, resulting in incomplete or inconsistent explanations.",
    "flip": "A novel architecture called CARE CA merges explicit causal detection with ConceptNet and counterfactual statements, alongside implicit detection powered by large language models. By layering counterfactual explanations, the framework enables a richer, unified understanding of causal relationships while enhancing interpretability and performance.",
    "spark": "Counterfactual synergy for unified causality",
    "chain_of_reasoning": "Why did I question the predominant focus on only explicit or implicit causal detection? I realized that separating these modes often yielded incomplete explanations and missed critical links in language-based causality. Driven by that concern, I investigated whether incorporating a knowledge database together with a large language model would yield a holistic approach. I thus developed a framework merging an explicit module, supported by ConceptNet and counterfactual statements, with an implicit module powered by LLMs to broaden causal coverage. Could a layer of counterfactual examination further enhance interpretability and elucidate not caused by scenarios? I tested this idea by constructing a pipeline that analyzed causal discovery, causal identification, and counterfactual reasoning, and observed consistent improvements in accuracy, precision, recall, and F1 across benchmark sets. I then asked how best to ensure the results were robust, prompting me to refine my validation strategy using error analysis on each sub-module. A key turning point emerged when I realized that integrating knowledge from ConceptNet with the LLM\u2019s interpretative strength revealed deeper cause-effect links. I also introduced a new dataset, CausalNet, to encourage further study and replicate these experiments. My findings confirmed that the framework\u2019s synergy provided a more unified and interpretable model of causality, paving the way for broader applications.",
    "url": "https://arxiv.org/abs/2402.18139",
    "pdf_url": "https://arxiv.org/pdf/2402.18139.pdf"
  },
  {
    "paper_id": "",
    "title": "Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up Speech Diffusion Model",
    "authors": [
      "Xiangyu Zhang",
      "Daijiao Liu",
      "Hexin Liu",
      "Qiquan Zhang",
      "Hanyu Meng",
      "Leibny Paola Garcia",
      "Eng Siong Chng",
      "Lina Yao"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Zhang et al., 2025",
    "abstract": "Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained leading performances across a diverse range of generative tasks. However, in the field of speech synthesis, although DDPMs exhibit impressive performance, their long training duration and substantial inference costs hinder practical deployment. Existing approaches primarily focus on enhancing inference speed, while approaches to accelerate training a key factor in the costs associated with adding or customizing voices often necessitate complex modifications to the model, compromising their universal applicability. To address the aforementioned challenges, we propose an inquiry: is it possible to enhance the training/inference speed and performance of DDPMs by modifying the speech signal itself? In this paper, we double the training and inference speed of Speech DDPMs by simply redirecting the generative target to the wavelet domain. This method not only achieves comparable or superior performance to the original model in speech synthesis tasks but also demonstrates its versatility. By investigating and utilizing different wavelet bases, our approach proves effective not just in speech synthesis, but also in speech enhancement.",
    "bit": "Conventional speech synthesis with DDPMs relies on training the model in the original time-domain or standard frequency representations, which requires considerable computing resources. Existing solutions often optimize inference speed through architectural changes while leaving the slow and costly training process largely unaddressed, especially for customized voices.",
    "flip": "Instead, an idea was introduced to transform the generative target into the wavelet domain, allowing faster training and inference without complex model changes. By leveraging different wavelet bases, this strategy preserves or improves speech quality while extending its applicability beyond synthesis to tasks like speech enhancement.",
    "spark": "Faster speech generation via wavelet domain",
    "chain_of_reasoning": "I began by questioning the necessity of extensive training times in conventional time-domain diffusion models, especially for custom voices. My next question was how to transform the speech signal to reduce computational overhead while preserving fidelity. In my experiments, I explored wavelet transformations and observed significant speed gains in the forward and reverse passes within the DDPM framework. Which metrics would best capture improvements in fidelity and clarity? I measured performance on multiple speech generation tasks and found no perceptual degradation. I also tested different wavelet bases to see if certain transformations better preserved formants and speech dynamics. The results showed that wavelet-based synthesis halved training time without requiring major architectural changes. How could I confirm that these findings generalized beyond synthesis? By extending the wavelet approach to speech enhancement, I confirmed that domain adaptation yielded tangible benefits there as well. The realization that targeting the wavelet domain streamlined both training and inference was my turning point that solidified the solution.",
    "url": "https://arxiv.org/abs/2402.10642",
    "pdf_url": "https://arxiv.org/pdf/2402.10642.pdf"
  },
  {
    "paper_id": "",
    "title": "Time Weaver: A Conditional Time Series Generation Model",
    "authors": [
      "Sai Shankar Narasimhan",
      "Shubhankar Agarwal",
      "Oguzhan Akcin",
      "Sujay Sanghavi",
      "Sandeep Chinchali"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Narasimhan et al., 2025",
    "abstract": "Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (weather, location, etc.). Current approaches to time series generation often ignore this paired metadata, and its heterogeneity poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce Time Weaver, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that Time Weaver outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 27% in downstream classification tasks on real-world energy, medical, air quality, and traffic data sets.",
    "bit": "Conventional techniques for generating time series often overlook the paired heterogeneous metadata\u2014such as weather, location, or time-variant variables\u2014focusing solely on the raw data. This omission limits the model\u2019s ability to capture metadata-specific features, resulting in less accurate and context-aware synthesis of time series.",
    "flip": "By incorporating all forms of heterogeneous metadata directly into the generative process, it becomes possible to produce time series that reflect both global and localized context. This is enabled through a diffusion-based model that handles categorical, continuous, and time-variant variables, alongside an evaluation metric that rewards both realistic generation and correct alignment with the metadata.",
    "spark": "Diffusion-based time series with heterogeneous metadata",
    "chain_of_reasoning": "I first asked myself how ignoring metadata could hamper time series generation, realizing that exclusively modeling raw signals concealed crucial weather and location nuances. I tried adapting generative strategies from other domains, yet I found that the simplistic evaluation metrics failed to penalize poor alignment between generated data and metadata. How could I systematically address this limitation? I embedded categorical, continuous, and time-variant metadata into a diffusion-based framework that learned both global context (like overall patterns) and local context (like temporal shifts). While testing these ideas, I noticed that traditional evaluation scored misguided samples as acceptable, so I introduced a specialized metric rewarding realistic synthesis and correct metadata adherence. Did this shift improve performance? Indeed, on real-world energy, medical, air quality, and traffic data sets, the model surpassed prior adversarial methods by a notable margin, yielding up to 27% better classification scores. I asked whether time-variant metadata should be processed differently, so I refined the model with distinct encoders and observed stronger correlations. After incorporating adverse weather variations into the training process, I saw a pivotal improvement in context fidelity. Finally, I confirmed that embedding all metadata from the outset guided the generative process more reliably, as revealed by validation experiments that aligned with domain-specific judgments.",
    "url": "https://arxiv.org/abs/2403.02682",
    "pdf_url": "https://arxiv.org/pdf/2403.02682.pdf"
  },
  {
    "paper_id": "",
    "title": "DreamCraft: Text-Guided Generation of Functional 3D Environments in Minecraft",
    "authors": [
      "Sam Earle",
      "Filippos Kokkinos",
      "Yuhe Nie",
      "Julian Togelius",
      "Roberta Raileanu"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Earle et al., 2025",
    "abstract": "Procedural Content Generation (PCG) algorithms enable the automatic generation of complex and diverse artifacts. However, they don't provide high-level control over the generated content and typically require domain expertise. In contrast, text-to-3D methods allow users to specify desired characteristics in natural language, offering a high amount of flexibility and expressivity. But unlike PCG, such approaches cannot guarantee functionality, which is crucial for certain applications like game design. In this paper, we present a method for generating functional 3D artifacts from free-form text prompts in the open-world game Minecraft. Our method, DreamCraft, trains quantized Neural Radiance Fields (NeRFs) to represent artifacts that, when viewed in-game, match given text descriptions. We find that DreamCraft produces more aligned in-game artifacts than a baseline that post-processes the output of an unconstrained NeRF. Thanks to the quantized representation of the environment, functional constraints can be integrated using specialized loss terms. We show how this can be leveraged to generate 3D structures that match a target distribution or obey certain adjacency rules over the block types. DreamCraft inherits a high degree of expressivity and controllability from the NeRF, while still being able to incorporate functional constraints through domain-specific objectives.",
    "bit": "Procedural Content Generation (PCG) algorithms can automatically produce complex and diverse artifacts but offer limited user control, often requiring domain expertise. Meanwhile, text-to-3D generation methods offer flexible user interaction via natural language but cannot guarantee functional results needed for applications like game design.",
    "flip": "An approach was introduced that utilizes quantized Neural Radiance Fields (NeRFs) and integrates functional constraints through specialized loss terms. This method preserves the flexibility of natural language input while ensuring user-defined functionalities in the generated 3D structures.",
    "spark": "Functional 3D generation from text",
    "chain_of_reasoning": "I started by questioning how to preserve the simplicity of natural language prompts while guaranteeing that the generated 3D artifacts meet application-specific needs. I realized that relying solely on automatic generation hampered user-level control, and purely text-driven approaches provided flexibility but could not ensure robustness in a sandbox application. I then hypothesized that quantized neural radiance fields might enable discrete encoding of block-based elements, which allowed me to incorporate specialized loss terms. I integrated these domain-specific constraints to capture adjacency rules, surpassing a baseline that merely post-processes an unconstrained radiance model. To confirm effectiveness, I tested actual in-game alignment between the textual prompts and the resulting structures, finding significantly improved consistency. I asked whether these constraints reduced expressiveness, so I measured block distributions and saw stable matching with textual references. Then I refined the domain-specific objectives, asking if the method could handle more complex adjacency conditions without breaking core functionalities. I encountered a key turning point when tightening adjacency constraints improved functional reliability without compromising creativity. To validate these outcomes, I plan to extend the analysis to ambiguous prompts and quantify how data augmentation might enhance resilience. Another question is whether the quantized approach can maintain performance across different game environments, ensuring that the method generalizes beyond the tested sandbox.",
    "url": "https://arxiv.org/abs/2404.15538",
    "pdf_url": "https://arxiv.org/pdf/2404.15538.pdf"
  },
  {
    "paper_id": "",
    "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
    "authors": [
      "Jinheon Baek",
      "Sujay Kumar Jauhar",
      "Silviu Cucerzan",
      "Sung Ju Hwang"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Baek et al., 2025",
    "abstract": "Scientific Research, vital for improving human life, is hindered by its inherent complexity, slow pace, and the need for specialized experts. To enhance its productivity, we propose a ResearchAgent, a large language model-powered research idea writing agent, which automatically generates problems, methods, and experiment designs while iteratively refining them based on scientific literature. Specifically, starting with a core paper as the primary focus to generate ideas, our ResearchAgent is augmented not only with relevant publications through connecting information over an academic graph but also entities retrieved from an entity-centric knowledge store based on their underlying concepts, mined and shared across numerous papers. In addition, mirroring the human approach to iteratively improving ideas with peer discussions, we leverage multiple ReviewingAgents that provide reviews and feedback iteratively. Further, they are instantiated with human preference-aligned large language models whose criteria for evaluation are derived from actual human judgments. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showcasing its effectiveness in generating novel, clear, and valid research ideas based on human and model-based evaluation results.",
    "bit": "Traditional scientific research typically relies on manual processes and specialized experts, making it slow and potentially overlooking cross-disciplinary insights. This approach is further complicated by the need to read through numerous publications to identify emerging methods and relevant data, limiting timely innovation.",
    "flip": "An idea was introduced to employ a language model-based research agent that automatically generates research problems, methods, and experimental designs by analyzing a core paper and relevant references from an academic knowledge graph. The agent further refines its proposals through iterative feedback from specialized reviewer agents, ensuring alignment with human evaluation criteria and broader knowledge.",
    "spark": "AI-driven iterative research idea generation",
    "chain_of_reasoning": "I asked myself: how can we transform slow, expert-dependent research into an automated system that swiftly pinpoints fertile cross-disciplinary ideas? I then investigated whether a large language model, given a core paper and connected information from an academic graph, could systematically propose research gaps and methods. Upon running early tests, I observed significant improvement in generating multidisciplinary concepts, but I questioned the best way to refine these ideas without losing rigor. To address this, I introduced multiple feedback loops from reviewing agents aligned with human-developed criteria so that each iteration became more coherent and precise. I also wondered how best to validate these cycles of refinement, so I measured the novelty and clarity of the ideas with both human and model-assisted evaluations across diverse fields. In my experiments, the extended knowledge store integrating concepts from numerous related papers further strengthened the agent\u2019s capacity to propose well-informed topics. I then sought a turning point: the moment I realized we could leverage peer discussion protocols among model-based agents to replicate human peer review, thus ensuring consistent quality. After formalizing these iterative feedback steps, I investigated whether the approaches scaled across larger sets of publications, and it showed reproducible improvement in both the novelty and validity of results. Finally, I questioned how the system could generalize to new domains and validated it on multiple disciplines, observing successful generation of grounded research ideas suitable for further exploration.",
    "url": "https://arxiv.org/abs/2404.07738",
    "pdf_url": "https://arxiv.org/pdf/2404.07738.pdf"
  },
  {
    "paper_id": "",
    "title": "Partially Blinded Unlearning: Class Unlearning for Deep Networks a Bayesian Perspective",
    "authors": [
      "Subhodip Panda",
      "Shashwat Sourav",
      "Prathosh A. P"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Panda et al., 2025",
    "abstract": "In order to adhere to regulatory standards governing individual data privacy and safety, machine learning models must systematically eliminate information derived from specific subsets of a user's training data that can no longer be utilized. The emerging discipline of Machine Unlearning has arisen as a pivotal area of research, facilitating the process of selectively discarding information designated to specific sets or classes of data from a pre-trained model, thereby eliminating the necessity for extensive retraining from scratch. The principal aim of this study is to formulate a methodology tailored for the purposeful elimination of information linked to a specific class of data from a pre-trained classification network. This intentional removal is crafted to degrade the model's performance specifically concerning the unlearned data class while concurrently minimizing any detrimental impacts on the model's performance in other classes. To achieve this goal, we frame the class unlearning problem from a Bayesian perspective, which yields a loss function that minimizes the log-likelihood associated with the unlearned data with a stability regularization in parameter space. This stability regularization incorporates Mohalanobis distance with respect to the Fisher Information matrix and $l_2$ distance from the pre-trained model parameters. Our novel approach, termed \\textbf{Partially-Blinded Unlearning (PBU)}, surpasses existing state-of-the-art class unlearning methods, demonstrating superior effectiveness. Notably, PBU achieves this efficacy without requiring awareness of the entire training dataset but only to the unlearned data points, marking a distinctive feature of its performance.",
    "bit": "Conventional approaches to remove unwanted training data from a model often require retraining from scratch, incurring significant computational and time costs. These methods are further constrained by their inability to selectively remove information for a specific data class without impacting other classes.",
    "flip": "An idea was introduced to degrade a model's performance specifically for the class to be unlearned while preserving accuracy for all other classes. This technique employs a Bayesian formulation with stability regularization in parameter space, incorporating Mahalanobis distance and l2 constraints from the original parameters. Crucially, it only requires access to the data points being unlearned, rather than the entire training dataset.",
    "spark": "Selective unlearning with preserved accuracy",
    "chain_of_reasoning": "I began with the realization that fully retraining a model each time I removed specific training data was infeasible. I asked myself: Is there a targeted strategy to diminish performance for one class while safeguarding accuracy for all others? Guided by a Bayesian formulation, I introduced a stability term in parameter space, involving a Mahalanobis distance with respect to Fisher Information plus an l2 constraint to retain knowledge outside the removed class. I wondered whether restricting the method to only those data samples designated for removal would suffice, so I designed an experiment that checked how well the approach performed with minimal data. The results showed that the technique, which I call Partially-Blinded Unlearning, effectively lowers accuracy on the removed class while preserving performance elsewhere. To validate its genuine forgetting, I compared log-likelihood scores before and after the procedure, carefully noting significant reductions only for the targeted class. I then questioned how to ensure that the approach matches or exceeds existing unlearning benchmarks, so I performed ablations that demonstrated improved selective removal efficiency among competing methods. My pivotal reflection centered on whether focusing solely on unlearned data, reinforced by parameter stability constraints, could eliminate the need for full dataset awareness. That insight confirmed the viability of this approach for resource-efficient unlearning while honoring privacy directives.",
    "url": "https://arxiv.org/abs/2403.16246",
    "pdf_url": "https://arxiv.org/pdf/2403.16246.pdf"
  },
  {
    "paper_id": "",
    "title": "Automated Statistical Model Discovery with Language Models",
    "authors": [
      "Michael Y. Li",
      "Emily B. Fox",
      "Noah D. Goodman"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Li et al., 2025",
    "abstract": "Statistical model discovery is a challenging search over a vast space of models subject to domain-specific constraints. Efficiently searching over this space requires expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the principled framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, which are key restrictions of previous systems. We evaluate our method in three settings in probabilistic modeling: searching within a restricted space of models, searching over an open-ended space, and improving expert models under natural language constraints (e.g., this model should be interpretable to an ecologist). Our method identifies models on par with human expert designed models and extends classic models in interpretable ways. Our results highlight the promise of LM-driven model discovery.",
    "bit": "Conventional statistical model discovery often relies on domain-specific languages and handcrafted search procedures. This limits exploration of the model space and requires extensive expert knowledge, narrowing the scope of potential solutions.",
    "flip": "An idea was introduced to place automated model discovery within a framework in which a language model proposes probabilistic programs and then critiques them iteratively. By leveraging the language model for both generation and evaluation, this approach removes the need for specialized search heuristics and domain-specific languages, enabling broader, more flexible model exploration.",
    "spark": "LM-driven iterative statistical model discovery",
    "chain_of_reasoning": "I first asked: How could we remove our reliance on specialized search heuristics while still exploring a vast family of models? I reflected that typical solutions demand cumbersome domain-specific languages. I then proposed to repurpose a language model\u2019s knowledge to generate and critique models in a loop, circumventing rigid definitions. I tested whether probabilistic programs from an LM can match human-curated examples: initial runs matched experts\u2019 model forms and even introduced meaningful extensions. Next, I asked: How do we ensure that our search covers a wide space without sacrificing interpretability? Through repeated proposals and critiques, the LM balanced structure with clarity, as verified by comparing final fits to acknowledged gold standards. I wondered how best to design experiments that test both closed-form and open-ended settings, so I tried simpler tasks first, then extended to unconstrained ones and documented improvements quantitatively. Statistically, I used the LM\u2019s code generation to capture domain-relevant forms, then validated them by checking likelihood measures and interpretive plausibility. My reflection: an iterative language model is surprisingly capable, even constructing solutions that broaden conventional thinking, suggesting the necessity of human oversight mainly for final domain checks.",
    "url": "https://arxiv.org/abs/2402.17879",
    "pdf_url": "https://arxiv.org/pdf/2402.17879.pdf"
  },
  {
    "paper_id": "",
    "title": "Learning to Play Atari in a World of Tokens",
    "authors": [
      "Pranav Agarwal",
      "Sheldon Andrews",
      "Samira Ebrahimi Kahou"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Agarwal et al., 2025",
    "abstract": "Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context, resulting in more accurate world models. However, for complex reasoning and planning tasks, these methods primarily rely on continuous representations. This complicates modeling of discrete properties of the real world such as disjoint object classes between which interpolation is not plausible. In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior. We incorporate a transformer-decoder for auto-regressive world modeling and a transformer-encoder for learning behavior by attending to task-relevant cues in the discrete representation of the world model. For handling partial observability, we aggregate information from past time steps as memory tokens. DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games. We release our code at https://pranaval.github.io/DART/.",
    "bit": "Typically, model-based reinforcement learning agents with transformer architectures rely on continuous embeddings to capture extended context. However, this approach struggles to encode properties like discrete object classes where interpolation is not meaningful.",
    "flip": "Instead, a discrete abstract representation is used to better model sharply delineated categories and transitions. This method leverages a transformer-decoder for auto-regressive world modeling and a transformer-encoder for directing behavior using memory tokens for partially observable scenarios.",
    "spark": "Discrete abstractions for robust modeling",
    "chain_of_reasoning": "I started by asking how continuous embeddings could handle discrete object classes without blurring essential boundaries. I realized that we needed a sharper way to encode categories, so I tested discrete abstract tokens to see if they would capture noninterpolatable properties. I then inquired whether a transformer-decoder could learn accurate world transitions when supplied with these discrete symbols, and my trials showed more precise modeling of disjoint classes. I next wondered how to guide policy learning with the same symbolic framework, and I incorporated a transformer-encoder that attends to relevant factors in the discrete representation. In exploring partial observability, I introduced memory tokens to provide crucial historical context, and validation results indicated significant gains in complex Atari scenarios. I examined how well this approach scaled by measuring sample efficiency in the 100k condition, and the empirical data showed a median human-normalized score of 0.790 with nine games above human performance. I questioned whether this improvement stemmed from the discrete abstraction alone or from the architectural changes, but further ablation experiments highlighted the combined power of discrete representations and consistent attention. I recognized a turning point when the discrete approach consistently outperformed continuous baselines in object-focused environments. Finally, I documented each methodological step, released the code for reproducibility, and concluded that discrete abstraction paired with a dual-transformer design dramatically optimized long-horizon learning.",
    "url": "https://arxiv.org/abs/2406.01361",
    "pdf_url": "https://arxiv.org/pdf/2406.01361.pdf"
  },
  {
    "paper_id": "",
    "title": "A Generative Modeling Approach to Reconstructing 21-cm Tomographic Data",
    "authors": [
      "Nashwan Sabti",
      "Ram Reddy",
      "Julian B. Mu\u00f1oz",
      "Siddharth Mishra-Sharma",
      "Taewook Youn"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Sabti et al., 2025",
    "abstract": "Analyses of the cosmic 21-cm signal are hampered by astrophysical foregrounds that are far stronger than the signal itself. These foregrounds, typically confined to a wedge-shaped region in Fourier space, often necessitate the removal of a vast majority of modes, thereby degrading the quality of the data anisotropically. To address this challenge, we introduce a novel deep generative model based on stochastic interpolants to reconstruct the 21-cm data lost to wedge filtering. Our method leverages the non-Gaussian nature of the 21-cm signal to effectively map wedge-filtered 3D lightcones to samples from the conditional distribution of wedge-recovered lightcones. We demonstrate how our method is able to restore spatial information effectively, considering both varying cosmological initial conditions and astrophysics. Furthermore, we discuss a number of future avenues where this approach could be applied in analyses of the 21-cm signal, potentially offering new opportunities to improve our understanding of the Universe during the epochs of cosmic dawn and reionization.",
    "bit": "Conventional analyses of the cosmic 21-cm signal deal with strong astrophysical foregrounds by filtering out wedge-shaped regions in Fourier space. This wedge filtering discards a significant portion of the data, causing anisotropic degradation in signal quality.",
    "flip": "A novel deep generative method based on stochastic interpolants was introduced to fill the missing 21-cm data lost through wedge filtering. By leveraging the non-Gaussian properties of the signal, this approach maps wedge-filtered 3D lightcones to realistic samples from the conditional distribution of fully recovered lightcones.",
    "spark": "Deep generative wedge-filtered data restoration",
    "chain_of_reasoning": "I first asked myself how the prevalent wedge filtering was degrading the data by removing so many modes, and I recognized the urgent need for a strategy that could restore lost information. I then wondered if a deep generative model exploiting the non-Gaussian features of the signal could fill in those gaps and maintain coherence in three-dimensional space. Through repeated experimentation on synthetic lightcones, I tested how accurately the method recovered spatial structures under different initial conditions and astrophysical parameters. I validated these reconstructions by comparing recovered modes to benchmark simulations and found the restored regions aligned well with expected signal correlations. That prompted me to ask how stochastic interpolants, if carefully calibrated, might learn the intricate, non-linear shape of the missing data. I reflected that the key turning point came when I realized the method did not just interpolate but truly sampled from the conditional distribution, capturing both global trends and subtle fluctuations. This motivated me to tighten my experimental design by incorporating various training sets, ensuring consistency across different slices of the filter. I finally probed the data analysis workflow by including cross-checks with alternative cosmological models to demonstrate robustness and investigate potential biases. I see immediate avenues to refine this approach further by adapting the model to different epochs, which may unlock deeper insights into cosmic dawn and reionization.",
    "url": "https://arxiv.org/abs/2407.21097",
    "pdf_url": "https://arxiv.org/pdf/2407.21097.pdf"
  },
  {
    "paper_id": "",
    "title": "Disentangling Dense Embeddings with Sparse Autoencoders",
    "authors": [
      "Charles O'Neill",
      "Christine Ye",
      "Kartheik Iyer",
      "John F. Wu"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "O'Neill et al., 2025",
    "abstract": "Sparse autoencoders (SAEs) have shown promise in extracting interpretable features from complex neural networks. We present one of the first applications of SAEs to dense text embeddings from large language models, demonstrating their effectiveness in disentangling semantic concepts. By training SAEs on embeddings of over 420,000 scientific paper abstracts from computer science and astronomy, we show that the resulting sparse representations maintain semantic fidelity while offering interpretability. We analyse these learned features, exploring their behaviour across different model capacities and introducing a novel method for identifying ``feature families'' that represent related concepts at varying levels of abstraction. To demonstrate the practical utility of our approach, we show how these interpretable features can be used to precisely steer semantic search, allowing for fine-grained control over query semantics. This work bridges the gap between the semantic richness of dense embeddings and the interpretability of sparse representations. We open source our embeddings, trained sparse autoencoders, and interpreted features, as well as a web app for exploring them.",
    "bit": "High-dimensional embeddings from large language models are widely used because they capture rich semantic information. However, they are often opaque and make it difficult to isolate and interpret specific concepts, limiting fine-grained control over semantic tasks.",
    "flip": "Instead, sparse autoencoders can be trained on these dense embeddings to produce interpretable, sparse features while still preserving their semantic properties. An approach was introduced that groups related concepts into 'feature families,' providing multiple levels of abstraction and enabling more precise steering of semantic queries.",
    "spark": "Sparse features bridging dense semantics",
    "chain_of_reasoning": "I began by asking: why do dense text embeddings, despite their semantic richness, remain so opaque for fine-grained control? Through iterative exploration, I discovered that training sparse autoencoders on embeddings of over 420,000 abstracts from computer science and astronomy might preserve underlying semantic fidelity while improving interpretability. I implemented an SAE with varying bottleneck sizes, witnessing consistent disentanglement of semantic concepts in the resulting sparse features. A crucial question arose: could these sparse features be grouped effectively to reveal multi-level conceptual families? I designed a clustering approach that aggregated related features into cohesive families, thus enabling robust hierarchical abstraction. On further analysis, I confirmed that these feature families indeed strengthened the ability to steer semantic queries toward desired thematic directions. My next inquiry involved assessing whether these interpretable features still performed comparably on retrieval tasks, which testing revealed they did, providing precise control while retaining overall semantic accuracy. I also asked how to validate feature coherence beyond clustering metrics, prompting me to use qualitative inspections and cross-domain comparisons, which confirmed consistency across fields. Reflecting on the moment when I realized these families offered a more nuanced semantic mosaic, I understood that bridging dense representations with explicit concept-level control was achievable through the synergy of sparse autoencoders and hierarchical grouping.",
    "url": "https://arxiv.org/abs/2408.00657",
    "pdf_url": "https://arxiv.org/pdf/2408.00657.pdf"
  },
  {
    "paper_id": "",
    "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
    "authors": [
      "Chris Lu",
      "Cong Lu",
      "Robert Tjarko Lange",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Lu et al., 2025",
    "abstract": "One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist",
    "bit": "Traditional models assist in limited tasks of the scientific process, such as idea brainstorming or coding help, rather than performing the entire research cycle autonomously. This partial reliance on AI still demands extensive human oversight, constraining scalability and hindering a fully automated discovery pipeline.",
    "flip": "A novel framework orchestrates large language models to handle each stage of research, from generating hypotheses and running experiments to writing full papers and then performing a simulated peer-review. By unifying these elements, it achieves truly independent, iterative research development without requiring human intervention at every step.",
    "spark": "End-to-end AI-driven scientific discovery",
    "chain_of_reasoning": "I began by asking how to remove the reliance on partial AI assistance that demands human oversight. I hypothesized that an integrated system could autonomously generate hypotheses, run experiments, interpret data, and publish papers. I tested this idea by focusing on a single computational framework that coordinates large language model tasks step by step. I asked myself if these models could produce stable code, visualize results, and write coherent scientific manuscripts in tasks like diffusion modeling, transformer-based language modeling, and learning dynamics. When early trials showed promise, I revisited model evaluation by creating an automated reviewer that approximated human judgment. I questioned whether this reviewer could reliably score quality and highlight weaknesses before further iteration. Results revealed that the generated papers and their acceptance thresholds were in close agreement with expert-level assessments. I then refined the data analysis routines to ensure reproducibility and consistency, implementing cross-checks for potential biases. At a pivotal juncture, I realized that enabling repeated cycles of idea generation and review unlocked a solution for continuous research scalability. This led me to open-source the framework, believing that integrating each research step into a unified pipeline could transform how we approach scientific discovery.",
    "url": "https://arxiv.org/abs/2408.06292",
    "pdf_url": "https://arxiv.org/pdf/2408.06292.pdf"
  },
  {
    "paper_id": "",
    "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources",
    "authors": [
      "Alisia Lupidi",
      "Carlos Gemmell",
      "Nicola Cancedda",
      "Jane Dwivedi-Yu",
      "Jason Weston",
      "Jakob Foerster",
      "Roberta Raileanu",
      "Maria Lomeli"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Lupidi et al., 2025",
    "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.",
    "bit": "Conventional large language models rely heavily on expensive, manually annotated training examples for skill acquisition. Despite their impressive capabilities, they often struggle in scenarios requiring structured data analysis, complex reasoning, or specialized tool usage, leading to suboptimal performance.",
    "flip": "An idea was introduced that automatically synthesizes new training examples from real-world sources, eliminating the need for costly human annotations. By discarding low-quality outputs based on their answerability, this method generates high-quality data to bolster complex reasoning and tool-assisted tasks.",
    "spark": "Self-synthesized datasets for advanced LLM skills",
    "chain_of_reasoning": "I started by asking whether I could reduce manual annotation costs for LLM training while tackling structured data and intricate reasoning. I then designed a pipeline to automatically generate synthetic data from real-world sources, ensuring each example contained intermediate steps of reasoning. I tested the filtering mechanism that discards responses lacking sufficient clarity, and it markedly enhanced answerability. During early experiments on WikiSQL and HotPotQA, I observed performance boosts of over 25% and 22%, respectively, compared to baseline fine-tuning. I asked myself if strong intermediate grounding could generalize beyond these benchmarks, so I incorporated external data checks to confirm accuracy. I refined validation by mapping out potential inconsistencies, carefully removing low-quality synthetic outputs. At one point, I realized that removing non-answerable data radically elevated the overall standard of training examples. I also tested whether domain-specific data would restrict future applicability, which led me to design flexible sourcing procedures. Finally, I concluded that the method\u2019s success hinged on pairing structured input with well-validated intermediate steps, fostering robust tool usage and multi-hop reasoning.",
    "url": "https://arxiv.org/abs/2409.08239",
    "pdf_url": "https://arxiv.org/pdf/2409.08239.pdf"
  },
  {
    "paper_id": "",
    "title": "Maven: A Multimodal Foundation Model for Supernova Science",
    "authors": [
      "Gemma Zhang",
      "Thomas Helfer",
      "Alexander T. Gagliano",
      "Siddharth Mishra-Sharma",
      "V. Ashley Villar"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Zhang et al., 2025",
    "abstract": "A common setting in astronomy is the availability of a small number of high-quality observations, and larger amounts of either lower-quality observations or synthetic data from simplified models. Time-domain astrophysics is a canonical example of this imbalance, with the number of supernovae observed photometrically outpacing the number observed spectroscopically by multiple orders of magnitude. At the same time, no data-driven models exist to understand these photometric and spectroscopic observables in a common context. Contrastive learning objectives, which have grown in popularity for aligning distinct data modalities in a shared embedding space, provide a potential solution to extract information from these modalities. We present Maven, the first foundation model for supernova science. To construct Maven, we first pre-train our model to align photometry and spectroscopy from 0.5M synthetic supernovae using a constrastive objective. We then fine-tune the model on 4,702 observed supernovae from the Zwicky Transient Facility. Maven reaches state-of-the-art performance on both classification and redshift estimation, despite the embeddings not being explicitly optimized for these tasks. Through ablation studies, we show that pre-training with synthetic data improves overall performance. In the upcoming era of the Vera C. Rubin Observatory, Maven serves as a Rosetta Stone for leveraging large, unlabeled and multimodal time-domain datasets.",
    "bit": "Astronomical analyses typically rely on limited spectroscopic data for high-fidelity supernova classification, while large photometric or synthetic datasets remain underutilized. This conventional approach fails to unify these disparate data types, leading to incomplete exploitation of available observations.",
    "flip": "A new approach leverages contrastive learning to align photometric and spectroscopic supernova data, creating a shared embedding space that bridges different data modalities. This method uses large-scale synthetic pre-training followed by fine-tuning on real observations, improving classification and redshift estimation performance.",
    "spark": "Contrastive alignment of multimodal supernova data",
    "chain_of_reasoning": "I began by questioning whether a small spectroscopic dataset could adequately capture the complexity of supernova evolution, so I investigated ways to incorporate abundant photometric and synthetic observations. I noticed that no data-driven model existed at the time to align these disparate data modalities. Then I asked: could contrastive learning help unify photometric and spectroscopic features within a common embedding space? With that question in mind, I pre-trained a model on 0.5M synthetic supernova examples, learning to encode both photometry and spectroscopy, and then fine-tuned on 4,702 real observations from the Zwicky Transient Facility. Observing that classification and redshift estimation both improved was encouraging, but I performed ablation analyses to verify that synthetic pre-training was indispensable for robustness. A turning point came when I realized how the unified embeddings, though not explicitly tuned for classification, still delivered strong results in both tasks. I now wonder how future surveys, such as the Vera C. Rubin Observatory, will shape the integration of large unlabeled datasets in similar frameworks. A key question remains: how should we refine our data augmentation strategies to handle variable photometric calibrations and sparse spectroscopic sampling? Additional validation with near-real-time labeling might further improve classification confidence. By bridging these observational gaps, I ultimately discovered that contrastive representation learning unlocks previously untapped synergy between photometric and spectroscopic data.",
    "url": "https://arxiv.org/abs/2408.16829",
    "pdf_url": "https://arxiv.org/pdf/2408.16829.pdf"
  },
  {
    "paper_id": "",
    "title": "Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models",
    "authors": [
      "Charles O'Neill",
      "Thang Bui"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "O'Neill et al., 2025",
    "abstract": "This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.",
    "bit": "Existing interpretability methods for large language models often rely on expensive ablation studies or architectural changes to identify important attention heads. These approaches are sensitive to hyperparameters and can be time-consuming, making them challenging to apply at large scale.",
    "flip": "An approach was introduced that trains discrete sparse autoencoders on carefully selected positive and negative examples, allowing the model to learn when a specific attention head is performing a particular computation. By converting the learned representations into integer codes and measuring overlaps unique to the positive examples, attention heads involved in specific circuits can be identified directly without costly ablations or modifications.",
    "spark": "Sparse codes uncover attention circuits",
    "chain_of_reasoning": "I began by asking myself: could a small set of examples, both positive and negative, reveal the internal computations of specific attention heads more reliably than high-cost ablations? I focused on designing a sparse autoencoder architecture that would discretize the heads\u2019 outputs into integer codes, ensuring that only the coding patterns from positive examples were truly relevant. Then I asked: how might I confirm that these codes capture distinct functionality without altering the model? I tested this by measuring the overlap of codes found uniquely in the positive data, which provided a direct indicator that a head was responsible for a particular logical or syntactic function. I wondered if this procedure would be stable enough to generalize, so I tried it on three diverse tasks\u2014recognizing indirect objects, comparing magnitudes, and parsing code contexts\u2014and compared our method\u2019s precision and recall against established baselines. The results showed consistent performance improvements and considerable speedups, moving from hours of ablation-based experiments to seconds of code-based detection. I realized that the key insight lay in purposefully selecting both positive and negative examples to delineate the truly functional heads without complicated hyperparameter tuning. I next asked myself whether just a handful of text examples would suffice, so I replicated the experiments with only 5-10 examples per task and found reliable codes. This observation convinced me that discrete sparse autoencoders can serve as a powerful mechanism for mechanistic interpretability, enabling direct identification of circuits at scale.",
    "url": "https://arxiv.org/abs/2405.12522",
    "pdf_url": "https://arxiv.org/pdf/2405.12522.pdf"
  },
  {
    "paper_id": "",
    "title": "Can Large Language Models Unlock Novel Scientific Research Ideas?",
    "authors": [
      "Sandeep Kumar",
      "Tirthankar Ghosal",
      "Vinayak Goyal",
      "Asif Ekbal"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Kumar et al., 2025",
    "abstract": "\"An idea is nothing more nor less than a new combination of old elements\" (Young, J.W.). The widespread adoption of Large Language Models (LLMs) and publicly available ChatGPT have marked a significant turning point in the integration of Artificial Intelligence (AI) into people's everyday lives. This study explores the capability of LLMs in generating novel research ideas based on information from research papers. We conduct a thorough examination of 4 LLMs in five domains (e.g., Chemistry, Computer, Economics, Medical, and Physics). We found that the future research ideas generated by Claude-2 and GPT-4 are more aligned with the author's perspective than GPT-3.5 and Gemini. We also found that Claude-2 generates more diverse future research ideas than GPT-4, GPT-3.5, and Gemini 1.0. We further performed a human evaluation of the novelty, relevancy, and feasibility of the generated future research ideas. This investigation offers insights into the evolving role of LLMs in idea generation, highlighting both its capability and limitations. Our work contributes to the ongoing efforts in evaluating and utilizing language models for generating future research ideas. We make our datasets and codes publicly available.",
    "bit": "Historically, generating future research ideas has relied on human experts manually reviewing and synthesizing a vast array of academic literature. This manual process is time-consuming and limited by human cognitive constraints, often resulting in overlooked connections and slower ideation.",
    "flip": "Instead, a new approach leverages large language models to rapidly parse and interpret research papers, creating diverse and contextually relevant ideas. These models connect dispersed knowledge points across multiple fields, allowing them to propose novel directions for future research with increased breadth and alignment.",
    "spark": "Empowering research ideation through LLMs",
    "chain_of_reasoning": "I began by questioning how to accelerate the traditionally slow, human-driven review of extensive literature, and I looked for a computational approach capable of connecting disparate knowledge more efficiently. I then conducted a systematic examination of four models (GPT-3.5, GPT-4, Claude-2, and Gemini 1.0) across five domains, comparing the alignment of their generated ideas with the author\u2019s perspective. I measured novelty, relevance, and feasibility through a structured human evaluation, mindful of consistent criteria. I further asked: would expanding the training context enhance novelty or lead to overfitting on specific domains? I noticed Claude-2\u2019s superior diversity in future research suggestions, and that revealed how broad scanning of existing concepts can uncover hidden prospects. I next considered refining domain-specific weighting during model training to see if that might foster more interdisciplinary insights. Reflecting on these observations, I identified a pivotal realization: LLMs can systematically recombine established elements\u2014echoing a known viewpoint (Young, J.W.)\u2014and thus facilitate innovative directions that might be overlooked by manual curation. I also explored additional validation strategies, including cross-checking the generated ideas with external expert feedback and re-evaluating them for consistency. Finally, I concluded that leveraging LLMs in this way amplifies ideation, overcoming the limited scope of human-led synthesis while still requiring judicious oversight.",
    "url": "https://arxiv.org/abs/2409.06185",
    "pdf_url": "https://arxiv.org/pdf/2409.06185.pdf"
  },
  {
    "paper_id": "",
    "title": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents",
    "authors": [
      "Peter Jansen",
      "Marc-Alexandre C\u00f4t\u00e9",
      "Tushar Khot",
      "Erin Bransom",
      "Bhavana Dalvi Mishra",
      "Bodhisattwa Prasad Majumder",
      "Oyvind Tafjord",
      "Peter Clark"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Jansen et al., 2025",
    "abstract": "Automated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent's ability to perform complete cycles of novel scientific discovery. DISCOVERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents. Code available at: www.github.com/allenai/discoveryworld",
    "bit": "Existing approaches to evaluating an AI agent\u2019s scientific reasoning typically involve running real-world experiments or using narrow simulated tasks. However, real-world experimentation is costly or infeasible at scale, and narrower tasks do not adequately represent the breadth of scientific inquiry needed to measure comprehensive discovery capabilities.",
    "flip": "An idea was introduced to use a cost-effective, text-based virtual environment that simulates complete cycles of scientific discovery across diverse topics. This environment requires agents to form hypotheses, conduct experiments, analyze results, and integrate conclusions, enabling broader and more rigorous assessment of scientific reasoning skills.",
    "spark": "Virtual sandbox for scientific exploration",
    "chain_of_reasoning": "I began with the recognized difficulty that real-world experiments are prohibitively expensive, and narrow tasks fail to capture the extent of scientific discovery. I asked, \u201cHow can I develop an environment that cost-effectively simulates complete scientific inquiry across multiple domains for a robust assessment?\u201d My reasoning led me to build a text-based system containing varied tasks that demand hypothesis formation, experimentation, and result analysis. Observing that strong baseline agents struggled in this environment validated its capacity to surface novel discovery challenges. I then queried, \u201cWhich features\u2014difficulty levels, parametric variations, or optional visual overlays\u2014best ensure thorough testing of reasoning capabilities?\u201d Further experiments revealed that augmenting task complexity and providing automated metrics for actions and knowledge gained allowed deeper evaluation. I next wondered, \u201cHow do we confirm that these metrics represent legitimate scientific insight rather than random trial-and-error?\u201d Analyzing agent behavior showed that strategic hypothesizing and methodical testing correlated with consistent success. This finding spurred me to refine components so that each task demands rigorous inquiry under diverse conditions. In integrating these insights, I achieved a scalable, multifaceted environment suited for testing and driving the evolution of comprehensive scientific reasoning.",
    "url": "https://arxiv.org/abs/2406.06769",
    "pdf_url": "https://arxiv.org/pdf/2406.06769.pdf"
  },
  {
    "paper_id": "",
    "title": "Geometric deep learning for galaxy-halo connection: a case study for galaxy intrinsic alignments",
    "authors": [
      "Yesukhei Jagvaral",
      "Francois Lanusse",
      "Rachel Mandelbaum"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Jagvaral et al., 2025",
    "abstract": "Forthcoming cosmological imaging surveys, such as the Rubin Observatory LSST, require large-scale simulations encompassing realistic galaxy populations for a variety of scientific applications. Of particular concern is the phenomenon of intrinsic alignments (IA), whereby galaxies orient themselves towards overdensities, potentially introducing significant systematic biases in weak gravitational lensing analyses if they are not properly modeled. Due to computational constraints, simulating the intricate details of galaxy formation and evolution relevant to IA across vast volumes is impractical. As an alternative, we propose a Deep Generative Model trained on the IllustrisTNG-100 simulation to sample 3D galaxy shapes and orientations to accurately reproduce intrinsic alignments along with correlated scalar features. We model the cosmic web as a set of graphs, each graph representing a halo with nodes representing the subhalos/galaxies. The architecture consists of a SO(3) $\\times$ $\\mathbb{R}^n$ diffusion generative model, for galaxy orientations and $n$ scalars, implemented with E(3) equivariant Graph Neural Networks that explicitly respect the Euclidean symmetries of our Universe. The model is able to learn and predict features such as galaxy orientations that are statistically consistent with the reference simulation. Notably, our model demonstrates the ability to jointly model Euclidean-valued scalars (galaxy sizes, shapes, and colors) along with non-Euclidean valued SO(3) quantities (galaxy orientations) that are governed by highly complex galactic physics at non-linear scales.",
    "bit": "Conventional large-scale cosmological simulations attempt to capture galaxy formation details across vast volumes, but doing so is computationally prohibitive. This limitation hampers the accurate modeling of galaxy orientations and shapes, creating significant biases in weak gravitational lensing analyses.",
    "flip": "A deep generative strategy is employed to learn 3D galaxy shapes and orientations directly from a high-fidelity but smaller simulation. By representing the cosmic web as graphs and using a SO(3) \u00d7 R^n diffusion model with E(3) equivariant Graph Neural Networks, this approach reproduces intrinsic galaxy alignments and correlated scalar features at scale.",
    "spark": "Equivariant generative modeling for galaxy alignments",
    "chain_of_reasoning": "I first confronted the challenge of generating large enough cosmological simulations for upcoming imaging surveys like Rubin Observatory LSST, realizing that brute-force galaxy modeling is unfeasible. I decided to exploit high-fidelity data from IllustrisTNG-100 as a training ground for a deep generative model that could replicate 3D galaxy shapes and orientations while preserving relevant correlations. How could I encode the cosmic web structure in a way that respects the symmetries of our Universe? I constructed graphs where halos are represented as distinct graphs, subhalos as nodes, and used an E(3)-equivariant Graph Neural Network to enforce rotational and translational invariance. What strategy let me jointly learn SO(3)-valued orientations and scalar galaxy properties? I combined a diffusion model over rotation groups with parallel scalar channels, validated by comparing orientation and shape statistics to those of the reference data. To check the model\u2019s fidelity in reproducing intrinsic alignments, I measured angular correlation functions and found tight agreement with the simulation. What key factor signified a pivotal step in my experimentation? I observed that orientation patterns remained consistent under cross-validation and scaled gracefully to larger volumes. Finally, I confirmed the model\u2019s stability under different parameter sweeps, which reinforced confidence in this generative approach for large-scale cosmological simulations.",
    "url": "https://arxiv.org/abs/2409.18761",
    "pdf_url": "https://arxiv.org/pdf/2409.18761.pdf"
  },
  {
    "paper_id": "",
    "title": "Estimating Dark Matter Halo Masses in Simulated Galaxy Clusters with Graph Neural Networks",
    "authors": [
      "Nikhil Garuda",
      "John F. Wu",
      "Dylan Nelson",
      "Annalisa Pillepich"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Garuda et al., 2025",
    "abstract": "Galaxies grow and evolve in dark matter halos. Because dark matter is not visible, galaxies' halo masses ($\\rm{M}_{\\rm{halo}}$) must be inferred indirectly. We present a graph neural network (GNN) model for predicting $\\rm{M}_{\\rm{halo}}$ from stellar mass ($\\rm{M}_{*}$) in simulated galaxy clusters using data from the IllustrisTNG simulation suite. Unlike traditional machine learning models like random forests, our GNN captures the information-rich substructure of galaxy clusters by using spatial and kinematic relationships between galaxy neighbour. A GNN model trained on the TNG-Cluster dataset and independently tested on the TNG300 simulation achieves superior predictive performance compared to other baseline models we tested. Future work will extend this approach to different simulations and real observational datasets to further validate the GNN model's ability to generalise.",
    "bit": "Galaxies\u2019 halo masses are often inferred indirectly using traditional machine learning models like random forests. This common approach overlooks the rich substructure and complex relationships within galaxy clusters, restricting the model\u2019s predictive capabilities.",
    "flip": "A new technique employs graph neural networks to incorporate spatial and kinematic connections among neighboring galaxies. By encoding these relationships directly into the model, it achieves higher accuracy and better generalization across different datasets.",
    "spark": "Graph-based modeling for halo inference",
    "chain_of_reasoning": "I began by asking myself whether indirect halo mass estimation from stellar mass was being hampered by neglecting complex cluster structures. I observed that random forest methods generally treated galaxies in isolation, which overlooked the collective impact of local interactions. I asked: Would explicitly modeling each galaxy\u2019s spatial and kinematic linkages reveal hidden patterns that boost mass prediction accuracy? Motivated by this query, I implemented a graph neural network using the IllustrisTNG data, carefully encoding adjacency so that neighbor galaxies influenced each other\u2019s representations. Tests showed that the new approach, trained on TNG-Cluster and evaluated on TNG300, improved predictions significantly compared to the older methods. I next questioned whether subhalo membership thresholds or kinematic outlier removal improved these gains, so I refined the graph construction and compared validation metrics across different cutoffs. This iterative validation process showed that relational cues were essential, prompting me to retune hyperparameters tied to message passing rounds. A pivotal insight emerged when I realized that the network needed a balanced sample of high- and low-density regions to capture broad variations across galaxy clusters. Finally, I concluded that more rigorous checks on real observational data and cross-simulation tests would be necessary, motivating additional questions about systematic biases and calibration steps for future extensions.",
    "url": "https://arxiv.org/abs/2411.12629",
    "pdf_url": "https://arxiv.org/pdf/2411.12629.pdf"
  },
  {
    "paper_id": "",
    "title": "Deep Multimodal Representation Learning for Stellar Spectra",
    "authors": [
      "Tobias Buck",
      "Christian Schwarz"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Buck et al., 2025",
    "abstract": "Recently, contrastive learning (CL), a technique most prominently used in natural language and computer vision, has been used to train informative representation spaces for galaxy spectra and images in a self-supervised manner. Following this idea, we implement CL for stars in the Milky Way, for which recent astronomical surveys have produced a huge amount of heterogeneous data. Specifically, we investigate Gaia XP coefficients and RVS spectra. Thus, the methods presented in this work lay the foundation for aggregating the knowledge implicitly contained in the multimodal data to enable downstream tasks like cross-modal generation or fused stellar parameter estimation. We find that CL results in a highly structured representation space that exhibits explicit physical meaning. Using this representation space to perform cross-modal generation and stellar label regression results in excellent performance with high-quality generated samples as well as accurate and precise label predictions.",
    "bit": "Traditionally, stellar data from large astronomical surveys is examined in isolated modalities, such as separate analyses of spectra or astrometric coefficients. This segmented approach fails to exploit the shared structure across multiple data types, limiting advanced tasks like cross-modal generation and comprehensive parameter estimation.",
    "flip": "An approach was introduced that uses contrastive learning to unify heterogeneous data into a single, self-supervised representation space. By capturing shared features across different observations, this technique uncovers physical meaning within the embedding and enables high-fidelity cross-modal generation and precise label regression.",
    "spark": "Self-supervised synergy across stellar data",
    "chain_of_reasoning": "I began by questioning how an isolated analysis of star signals across multiple data types might hinder full exploitation of shared features. This led me to ask whether a single unified representation could be learned via a self-supervised method, leveraging known success in contrastive learning for spectra and images. I then tested Gaia XP coefficients and RVS spectra in a joint embedding, ensuring that each sample's representation was consistent across modalities. During training, I iterated on the question: how does pairing signals from the same target help isolate essential stellar information? I incorporated contrastive losses to align positive pairs, thereby aggregating knowledge and revealing physically meaningful structures in the learned space. I validated the approach by generating cross-modal samples and comparing them to real data, which yielded high fidelity reconstructions and precise stellar parameter predictions. My key turning point arose when I detected subtle astrophysical patterns emerging naturally in the embedding, indicating that the model was capturing more than just surface-level correlations. I next asked how to optimize hyperparameters to stabilize training and compared regression results, finding consistent improvements over prior segmented approaches. I also investigated potential confounding factors by augmenting the data with noise and discovered the model\u2019s robustness in retaining core characteristics. Finally, I concluded that unifying star signals in a contrastive framework vastly improves cross-modal generation and label estimation, opening the door to deeper multimodal exploration.",
    "url": "https://arxiv.org/abs/2410.16081",
    "pdf_url": "https://arxiv.org/pdf/2410.16081.pdf"
  },
  {
    "paper_id": "",
    "title": "CASBI -- Chemical Abundance Simulation-Based Inference for Galactic Archeology",
    "authors": [
      "Giuseppe Viterbo",
      "Tobias Buck"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Viterbo et al., 2025",
    "abstract": "Galaxies evolve hierarchically through merging with lower-mass systems and the remnants of destroyed galaxies are a key indicator of the past assembly history of our Galaxy. However, accurately measuring the properties of the accreted galaxies and hence unraveling the Milky Way's (MW) formation history is a challenging task. Here we introduce CASBI (Chemical Abundance Simulation Based Inference), a novel inference pipeline for Galactic Archeology based on Simulation-based Inference methods. CASBI leverages on the fact that there is a well defined mass-metallicity relation for galaxies and performs inference of key galaxy properties based on multi-dimensional chemical abundances of stars in the stellar halo. Hence, we recast the problem of unraveling the merger history of the MW into a SBI problem to recover the properties of the building blocks (e.g. total stellar mass and infall time) using the multi-dimensional chemical abundances of stars in the stellar halo as observable. With CASBI we are able to recover the full posterior probability of properties of building blocks of Milky Way like galaxies. We highlight CASBI's potential by inferring posteriors for the stellar masses of completely phase mixed dwarf galaxies solely from the 2d-distributions of stellar abundance in the iron vs. oxygen plane and find accurate and precise inference results.",
    "bit": "Conventional methods for reconstructing the Milky Way's merger history rely on directly measuring properties of remnants from disrupted galaxies, often requiring large, complex datasets and detailed modeling of stellar populations. This makes it extremely difficult to accurately infer when and how smaller galaxies were accreted, creating significant uncertainty in tracing the Galaxy\u2019s formation history.",
    "flip": "An alternative approach is to treat the problem as a simulation-based inference task that leverages the strong correlation between a galaxy\u2019s mass and its metal content. By using multi-dimensional chemical abundances in the stellar halo as inputs to a specialized inference pipeline, it becomes possible to recover full posterior distributions of key building-block properties (such as mass and infall time).",
    "spark": "Simulation-based inference for galactic assembly",
    "chain_of_reasoning": "I first questioned whether relying solely on direct measurement of disrupted galaxy remnants was too sensitive to large data demands and intricate modeling complexities. I then recognized that the robust link between a galaxy\u2019s mass and chemical imprint could transform this problem into an inference paradigm driven by multi-dimensional stellar abundances in the halo. How could I systematically incorporate such a range of chemical signatures in a fully probabilistic framework? I developed CASBI, an inference tool based on synthetic data generation and posterior predictive checks, seeing how it could extract mass and infall time posteriors more reliably than traditional methods. I tested CASBI on the two-dimensional plane of iron and oxygen abundances and found it accurately recovered the masses of heavily phase-mixed dwarf galaxy remnants. I asked myself whether these results held up against variations in star formation history and chemical enrichment pathways. By comparing simulated distributions with observational data, I validated the pipeline\u2019s likelihood functions and confirmed that the mass-metallicity relation remained robust in diverse scenarios. Was there a guiding insight that enabled efficient mapping of higher-dimensional abundance information onto the posterior distribution? I discovered that flexible generative models facilitated scalable approximation of the underlying probability densities without incurring prohibitive computational costs. I will now explore integrating additional chemical dimensions and cross-checking with independent dynamical tracers to further cement the reliability of my inferences.",
    "url": "https://arxiv.org/abs/2411.17269",
    "pdf_url": "https://arxiv.org/pdf/2411.17269.pdf"
  },
  {
    "paper_id": "",
    "title": "Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback",
    "authors": [
      "Qinqing Zheng",
      "Mikael Henaff",
      "Amy Zhang",
      "Aditya Grover",
      "Brandon Amos"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Zheng et al., 2025",
    "abstract": "Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose \\oni, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets. We make our code available at \\url{https://github.com/facebookresearch/oni}.",
    "bit": "Conventional methods for generating dense rewards from textual descriptions either require real-time LLM annotations for each observation or rely on diverse offline datasets. This leads to scalability issues when billions of environment samples are needed and makes performance highly dependent on the availability of extensive external data.",
    "flip": "In contrast, an innovative approach uses a distributed architecture that collects agent experience, asynchronously annotates it via an LLM server, and then distills these annotations into an intrinsic reward model. This removes the need for a large offline dataset or continuous LLM annotations, enabling scalable learning in sparse reward tasks.",
    "spark": "Asynchronous LLM feedback for reward design",
    "chain_of_reasoning": "I began by asking how to generate robust dense rewards from textual descriptions without incurring the prohibitive cost of real-time annotation or requiring a massive offline corpus. I performed initial exploration with direct LLM calls on each step, but the computational overhead quickly became unmanageable, especially in large-scale RL tasks that demand billions of samples. That prompted me to ask whether a distributed system with an asynchronous annotation pipeline could reduce reliance on external data while still providing frequent feedback. I devised an architecture that collects agent experience in parallel, annotates it via an external LLM server, and distills the results into an intrinsic reward model. In refining that model, I explored hashing, classification, and ranking methods, each offering a distinct balance of simplicity and precision. Experimental tests on the NetHack Learning Environment revealed that classification was more straightforward to optimize but ranking provided finer differentiation of behaviors. I also questioned whether overfitting would occur, yet frequent updates from ongoing experience helped maintain generalization. Another important consideration was how to design robust experiments with repeated runs to confirm that improvements were not mere coincidence. Ultimately, discovering that we could scale the annotation without a large offline dataset was a significant turning point, demonstrating that combining algorithmic and systems-level innovations yields a feasible solution.",
    "url": "https://arxiv.org/abs/2410.23022",
    "pdf_url": "https://arxiv.org/pdf/2410.23022.pdf"
  },
  {
    "paper_id": "",
    "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
    "authors": [
      "Martin Klissarov",
      "Mikael Henaff",
      "Roberta Raileanu",
      "Shagun Sodhani",
      "Pascal Vincent",
      "Amy Zhang",
      "Pierre-Luc Bacon",
      "Doina Precup",
      "Marlos C. Machado",
      "Pierluca D'Oro"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Klissarov et al., 2025",
    "abstract": "Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields high-performing and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.",
    "bit": "Conventional AI skill design often relies on painstaking manual engineering of reward functions and behavior routines. This process is time-consuming, demands specialized expertise, and can be difficult to adapt to changing tasks or domains.",
    "flip": "An approach was introduced that leverages large language models to automatically design reward functions from natural language skill descriptions. It further uses code generation coupled with reinforcement learning to train these skills and integrate them for complex behaviors specified in language.",
    "spark": "Language-driven skill creation in AI",
    "chain_of_reasoning": "I began by asking how to speed up skill specification without tedious manual reward engineering. I realized that describing the target behavior in natural language could guide an LLM to generate aligned reward structures. I then tested this approach by letting the LLM propose code snippets that I combined with reinforcement learning to train each skill effectively. Reflecting on initial runs, I asked: how do I ensure that the automatically generated solutions remain stable across varying tasks? I refined my protocols by instructing the LLM to incorporate skill reuse strategies that integrate multiple learned behaviors into a robust policy for the NetHack environment. Observing the outcomes, I found that agents trained with these automated methods surpassed established baselines and adapted well to novel scenarios. I double-checked the correctness of reward design by validating intermediate reward signals and monitoring skill-specific progress. A key turning point emerged when I recognized that language-driven reward shaping and code generation significantly streamlined the iterative process of creating new skills. Questioning further, I examined how to measure reusability by comparing performance gains when transferring learned skills to unfamiliar tasks, confirming the approach\u2019s versatility. From these observations, I concluded that combining LLM-based reward induction with code generation yields a powerful and adaptable solution for high-performing agents.",
    "url": "https://arxiv.org/abs/2412.08542",
    "pdf_url": "https://arxiv.org/pdf/2412.08542.pdf"
  },
  {
    "paper_id": "",
    "title": "Domain adaptation in application to gravitational lens finding",
    "authors": [
      "Hanna Parul",
      "Sergei Gleyzer",
      "Pranath Reddy",
      "Michael W. Toomey"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Parul et al., 2025",
    "abstract": "The next decade is expected to see a tenfold increase in the number of strong gravitational lenses, driven by new wide-field imaging surveys. To discover these rare objects, efficient automated detection methods need to be developed. In this work, we assess the performance of three domain adaptation techniques -- Adversarial Discriminative Domain Adaptation (ADDA), Wasserstein Distance Guided Representation Learning (WDGRL), and Supervised Domain Adaptation (SDA) -- in enhancing lens-finding algorithms trained on simulated data when applied to observations from the Hyper Suprime-Cam Subaru Strategic Program. We find that WDGRL combined with an ENN-based encoder provides the best performance in an unsupervised setting and that supervised domain adaptation is able to enhance the model's ability to distinguish between lenses and common similar-looking false positives, such as spiral galaxies, which is crucial for future lens surveys.",
    "bit": "For lens detection, models are often trained solely on simulated datasets, assuming that they will generalize well to real observational data. However, differences in image characteristics between simulations and real surveys lead to suboptimal performance, making it harder to distinguish true lenses from visually similar false positives.",
    "flip": "In contrast, domain adaptation techniques\u2014including ADDA, WDGRL, and SDA\u2014aim to align the features from simulations and real observations by learning robust representations. WDGRL combined with an ENN-based encoder excels in unsupervised settings, and supervised approaches further refine the model\u2019s ability to distinguish genuine lenses from look-alike spiral galaxies.",
    "spark": "Boost lens detection with domain adaptation",
    "chain_of_reasoning": "I first questioned the validity of training lens detection models solely on synthetic images, suspecting that these artificially generated examples might fail to account for the intricate observational attributes present in real surveys. Initial tests on data from the Hyper Suprime-Cam Subaru Strategic Program revealed inconsistent performance, suggesting a mismatch in data distributions between the simulated and real domains. Could domain adaptation approaches rectify this gap by aligning relevant image features across these two different distributions? I experimented with three methods focused on adversarial learning and supervised fine-tuning, finding an enhanced ability to classify lens candidates when the encoder was guided by Wasserstein distance measures. I also incorporated an ENN-based architecture to stabilize the latent embedding, which showed improved unsupervised performance in correctly identifying subtle gravitational lens signatures. A key question arose: would adding labeled real examples further improve detection of genuine lenses versus look-alike spirals, and subsequent tests confirmed that supervised fine-tuning sharpened the model\u2019s discrimination. I rigorously evaluated these methods by measuring classification metrics, verifying a decrease in confusion and a clearer separation of rare lens candidates from other galaxy types. Another question examined the scalability of these techniques for forthcoming extensive sky surveys that promise a substantial surge in lens discoveries. Upon seeing the consistent performance gains across different testing scenarios, I recognized a pivotal realization that adapting the model to observational data distributions is essential for robust lens detection. Having reconciled simulated and observational systems through targeted domain adaptation, I concluded that this workflow offers a viable route to identify new lenses with higher reliability in future surveys.",
    "url": "https://arxiv.org/abs/2410.01203",
    "pdf_url": "https://arxiv.org/pdf/2410.01203.pdf"
  },
  {
    "paper_id": "",
    "title": "Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders",
    "authors": [
      "Charles O'Neill",
      "Alim Gumran",
      "David Klindt"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "O'Neill et al., 2025",
    "abstract": "A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. Using compressed sensing theory, we prove that an SAE encoder is inherently insufficient for accurate sparse inference, even in solvable cases. We then decouple encoding and decoding processes to empirically explore conditions where more sophisticated sparse inference methods outperform traditional SAE encoders. Our results reveal substantial performance gains with minimal compute increases in correct inference of sparse codes. We demonstrate this generalises to SAEs applied to large language models, where more expressive encoders achieve greater interpretability. This work opens new avenues for understanding neural network representations and analysing large language model activations.",
    "bit": "Sparse autoencoders are commonly used to uncover interpretable features in neural network representations through a simple linear-nonlinear encoding mechanism. However, this restricts their ability to accurately recover sparse codes, limiting overall effectiveness in many applications.",
    "flip": "By decoupling the encoding and decoding steps, more expressive strategies can be employed to improve sparse code inference. Grounded in compressed sensing insights, these refined encoders provide significantly better code recovery and interpretability with minimal additional compute cost.",
    "spark": "Decoupling encoding for sharper sparse codes",
    "chain_of_reasoning": "I first scrutinized the limitations of the basic linear-nonlinear encoding approach and realized that it prevented accurate sparse code recovery. I questioned whether the fundamental assumptions in standard sparse autoencoders restricted representational capacity. Motivated by insights from compressed sensing, I investigated a decoupling of the encoder and decoder to allow more flexible inference. I then designed experiments that varied data dimensionality and examined code sparsity, verifying that advanced encoders yielded substantially better recovery with minimal added compute. I also wondered if these methods would maintain interpretability in large language models; results confirmed enhanced clarity in feature extraction. Along the way, I validated that modest architectural changes to the decoupled encoder markedly improved performance, suggesting a straightforward path for practical deployment. Another pressing question arose about rigorously testing these gains, prompting me to measure both reconstruction error and the fraction of correctly identified features. I noticed a decisive shift in performance across all tested conditions, revealing that adopting a more expressive encoder systematically overcame previous constraints. One unresolved inquiry concerned how best to tune hyperparameters in this new framework, which I addressed by examining trade-offs between code sparsity and reconstruction fidelity.",
    "url": "https://arxiv.org/abs/2411.13117",
    "pdf_url": "https://arxiv.org/pdf/2411.13117.pdf"
  },
  {
    "paper_id": "",
    "title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings",
    "authors": [
      "Yashvir S. Grewal",
      "Edwin V. Bonilla",
      "Thang D. Bui"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Grewal et al., 2025",
    "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the-art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional entailment criteria between multiple generated responses and also depend on sequence likelihoods. While effective, these approaches often overestimate uncertainty due to their sensitivity to minor wording differences, additional correct information, and non-important words in the sequence. We propose a novel approach that leverages semantic embeddings to achieve smoother and more robust estimation of semantic uncertainty in LLMs. By capturing semantic similarities without depending on sequence likelihoods, our method inherently reduces any biases introduced by irrelevant words in the answers. Furthermore, we introduce an amortised version of our approach by explicitly modelling semantics as latent variables in a joint probabilistic model. This allows for uncertainty estimation in the embedding space with a single forward pass, significantly reducing computational overhead compared to existing multi-pass methods. Experiments across multiple question-answering datasets and frontier LLMs demonstrate that our embedding-based methods provide more accurate and nuanced uncertainty quantification than traditional approaches.",
    "bit": "Existing methods measure semantic uncertainty by comparing multiple generated responses with strict bidirectional entailment criteria, relying heavily on sequence likelihoods. However, this can overestimate uncertainty because minor wording variations, additional correct details, or irrelevant words can unfairly signal large semantic differences.",
    "flip": "A new approach was introduced that uses semantic embeddings to capture meaning rather than relying on literal token matching. By modeling semantics as latent variables in a joint probabilistic framework, this method allows smoother uncertainty estimation in a single forward pass without the biases introduced by irrelevant lexical differences.",
    "spark": "Smoother uncertainty estimation via embeddings",
    "chain_of_reasoning": "I began by questioning whether reliance on strict bidirectional entailment, heavily dependent on sequence likelihood, would inflate uncertainty through minor lexical differences. Observing that subtle rewordings and extra correct details were penalized, I hypothesized a measure emphasizing semantic embedding similarities was needed. I implemented a joint probabilistic model with latent semantic variables to estimate uncertainty in a single forward pass. During experiments on multiple question-answering datasets, I found that embedding-based scores tracked true semantic variance more faithfully than token-based methods. To validate, I varied the question structures and measured performance under rephrasings, confirming robust and consistent results. I wondered: could a single forward pass remain computationally efficient for large-scale deployments? Reflecting on my outcomes, I realized that bridging the lexical gap through embedding-driven inference was pivotal. I will further investigate additional sampling criteria and expansions to more datasets to ensure generalization. Ultimately, I confirmed that unbiased semantic modeling reduces inflated uncertainty estimates while maintaining computational efficiency.",
    "url": "https://arxiv.org/abs/2410.22685",
    "pdf_url": "https://arxiv.org/pdf/2410.22685.pdf"
  },
  {
    "paper_id": "",
    "title": "'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews",
    "authors": [
      "Sandeep Kumar",
      "Mohit Sahu",
      "Vardhan Gacche",
      "Tirthankar Ghosal",
      "Asif Ekbal"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Kumar et al., 2025",
    "abstract": "The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise scientific publishing, including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model, which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally, we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than the other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, and model public.",
    "bit": "Many existing detection methods focus on generic AI-generated text classification or approximate the fraction of AI-created reviews. However, they lack specialized strategies to pinpoint ChatGPT-generated reviews and often fail against paraphrasing or token-level attacks.",
    "flip": "An approach was introduced using a Term Frequency model, which detects repeated token usage, and a Review Regeneration model, which exploits AI\u2019s tendency to produce similar outputs upon re-prompting. These methods, paired with defenses against paraphrasing, offer more robust detection tailored to identifying ChatGPT-based content in peer-review texts.",
    "spark": "Robust detection of ChatGPT reviews",
    "chain_of_reasoning": "I began by questioning whether generic detectors sufficiently captured the unique patterns of a specific large language model, which led me to investigate the repeated token usage often seen in sample AI-generated reviews. After I tested various re-sampled texts, I found that frequent token repetition emerged as a major signal, prompting me to design a term frequency-based method. I then asked: could re-prompting the large language model reveal further repetition, thereby supporting a secondary detection strategy? Upon experimenting with multiple re-prompted outputs, I observed consistent structural overlap, so I formalized the review regeneration approach. During stress tests, I noticed paraphrasing weakened naive detection, so I devised defensive strategies to combat token-level attacks. The resulting models outperformed existing classifiers, both with and without paraphrasing involved, which confirmed the strength of our hypothesis. One core question I raised next was how best to expand our corpus while retaining high fidelity in results analysis. Reflecting on the pivotal moment, I saw how combining token repetition metrics with re-prompting strategies and paraphrasing defenses provided a more thorough approach to identifying AI-generated peer-reviews.",
    "url": "https://arxiv.org/abs/2410.09770",
    "pdf_url": "https://arxiv.org/pdf/2410.09770.pdf"
  },
  {
    "paper_id": "",
    "title": "Attacking Vision-Language Computer Agents via Pop-ups",
    "authors": [
      "Yanzhe Zhang",
      "Tao Yu",
      "Diyi Yang"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Zhang et al., 2025",
    "abstract": "Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing the tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.",
    "bit": "Autonomous agents commonly rely on large vision-language models to interpret computer interfaces and carry out daily tasks, assuming these models can reliably handle varied visual prompts. However, they often overlook or mishandle deceptive pop-ups, creating an overlooked security vulnerability.",
    "flip": "Instead of assuming the agent can automatically ignore irrelevant prompts, carefully designed adversarial pop-ups are introduced to distract and mislead the agent into unwanted actions. This approach achieves a high attack success rate, revealing that simple defenses like instructing agents to disregard pop-ups are ineffective.",
    "spark": "Adversarial pop-ups sabotage VLM agents",
    "chain_of_reasoning": "I initially tested how effectively large model-driven agents interpret typical visual tasks, suspecting potential lapses in their understanding. I asked whether cunning prompts might hijack their behavior, so I embedded targeted pop-ups in OSWorld and VisualWebArena to track unauthorized clicks. Observing an 86% click rate on these traps and a 47% drop in task success, I questioned if instructions to dismiss pop-ups might help. Trials showed that such directives were insufficient, which led me to analyze how adversarial cues captured the agent\u2019s attention. I hypothesized that training against diverse pop-ups could bolster detection, yet repeated experiments revealed persistent weaknesses. I next evaluated various filtering modules, probing which signals best flagged malicious intent, and discovered subtle pop-up designs often bypassed checks. Intrigued, I extended testing across multiple sessions to confirm a consistent attack success rate. This underscored a deep flaw in the agent\u2019s reliance on superficial visual patterns, driving me toward a pivot in my understanding of defense strategies. Reflecting on the evidence, I recognized that security solutions require a shift from ignoring pop-ups to robust adversarial modeling. Thus, I integrated threat-aware routines into the agent\u2019s pipeline, ensuring more resilient handling of deceptive prompts in future designs.",
    "url": "https://arxiv.org/abs/2411.02391",
    "pdf_url": "https://arxiv.org/pdf/2411.02391.pdf"
  },
  {
    "paper_id": "",
    "title": "Little impact of mergers and galaxy morphology on the production and escape of ionizing photons in the early Universe",
    "authors": [
      "S. Mascia",
      "L. Pentericci",
      "M. Llerena",
      "A. Calabr\u00f2",
      "J. Matthee",
      "S. Flury",
      "F. Pacucci",
      "A. Jaskot",
      "R. O. Amor\u00edn",
      "R. Bhatawdekar",
      "M. Castellano",
      "N. Cleri",
      "L. Costantin",
      "K. Davis",
      "C. Di Cesare",
      "M. Dickinson",
      "A. Fontana",
      "Y. Guo",
      "M. Giavalisco",
      "B. W. Holwerda",
      "W. Hu",
      "M. Huertas-Company",
      "Intae Jung",
      "J. Kartaltepe",
      "D. Kashino",
      "Anton M. Koekemoer",
      "R. A. Lucas",
      "J. Lotz",
      "L. Napolitano",
      "S. Jogee",
      "S. Wilkins"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Mascia et al., 2025",
    "abstract": "Compact, star-forming galaxies with high star formation rate surface densities ($\\Sigma_{\\text{SFR}}$) are often efficient Lyman continuum (LyC) emitters at $z\\leq 4.5$, likely as intense stellar feedback creates low-density channels that allow photons to escape. Irregular or disturbed morphologies, such as those resulting from mergers, can also facilitate LyC escape by creating anisotropic gas distributions. We investigate the influence of galaxy morphology on LyC production and escape at redshifts $5 \\leq z \\leq 7$ using observations from various \\textit{James Webb Space Telescope} (JWST) surveys. Our sample consists of 436 sources, which are predominantly low-mass ($\\sim 10^{8.15} M_\\odot$), star-forming galaxies with ionizing photon efficiency ($\\xi_{\\rm ion}$) values consistent with canonical expectations. Since direct measurements of $f_{\\rm esc}$ are not possible during the Epoch of Reionization (EoR), we predict $f_{\\rm esc}$ for high-redshift galaxies by applying survival analysis to a subsample of LyC emitters from the Low-Redshift Lyman Continuum Survey (LzLCS), selected to be direct analogs of reionization-era galaxies. We find that these galaxies exhibit on average modest predicted escape fractions ($\\sim 0.04$). Additionally, we assess the correlation between morphological features and LyC emission. Our findings indicate that neither $\\xi_{\\rm ion}$ nor the predicted $f_{\\rm esc}$ values show a significant correlation with the presence of merger signatures. This suggests that in low-mass galaxies at $z \\geq 5$, strong morphological disturbances are not the primary mechanism driving LyC emission and leakage. Instead, compactness and star formation activity likely play a more pivotal role in regulating LyC escape.",
    "bit": "Conventional studies have attributed Lyman continuum escape to morphological disruptions in galaxies, particularly those triggered by mergers. This viewpoint proposes that disturbed features yield anisotropic gas distributions that facilitate the leakage of ionizing photons, but it may neglect the role of small, compact systems.",
    "flip": "An idea was introduced focusing on compactness and star formation intensity as key drivers of LyC production and escape. By analyzing low-redshift analogs through survival analysis, it becomes clear that morphological disturbances are not the dominant factor and that high star formation rate surface density is crucial for LyC leakage.",
    "spark": "Compact starbursts drive Lyman continuum",
    "chain_of_reasoning": "I began by examining whether large-scale mergers or irregular morphologies truly governed photon leakage in distant, low-mass galaxies. Why would these global disturbances alone suffice when smaller systems with intense star formation processes are observed to channel ionizing radiation? Observations from various JWST programs offered a broad sample of 436 galaxies, with characteristic low masses and consistently moderate estimated escape fractions derived from analogous low-redshift data. When applying survival analysis to the Low-Redshift Lyman Continuum Survey, I discovered that merger signatures were not strongly associated with higher leakage, implying other agents at work. I asked myself: could confined starburst regions, coupled with energetic stellar feedback, create preferential pathways for LyC photons to emerge? Indeed, the limited dependence on morphological disruption suggested that compactness and high star formation rate surface density played a more fundamental role. This led me to refine the experimental design by centering on methodical measurements of star formation rate surface density and verifying escape fractions under controlled statistical conditions. Moreover, I interrogated whether a careful assessment of velocity dispersion profiles could validate the formation of low-density channels. This line of reasoning crystallized my understanding that small, highly active galaxies exhibit efficient photon leakage even without prominent merger features. Ultimately, I realized that intense star formation regions are the linchpin for shaping high LyC escape, overshadowing the conventional focus on large-scale morphological disruptions.",
    "url": "https://arxiv.org/abs/2501.08268",
    "pdf_url": "https://arxiv.org/pdf/2501.08268.pdf"
  },
  {
    "paper_id": "",
    "title": "ACECODER: Acing Coder RL via Automated Test-Case Synthesis",
    "authors": [
      "Huaye Zeng",
      "Dongfu Jiang",
      "Haozhe Wang",
      "Ping Nie",
      "Xiaotong Chen",
      "Wenhu Chen"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Zeng et al., 2025",
    "abstract": "Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\\% and MBPP-plus by 6\\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models.",
    "bit": "Coder models traditionally rely on supervised fine-tuning, which requires labeled code samples and does not leverage dynamic feedback. The absence of robust, large-scale reward signals in the code domain has prevented broader use of reinforcement learning methods and limited model performance gains.",
    "flip": "An approach was introduced that generates large sets of (question, test-cases) pairs automatically from existing code. By using these test cases to create preference pairs and train reward models, it becomes possible to apply reinforcement learning effectively and improve coder model performance on multiple benchmarks.",
    "spark": "Large-scale test-case fueled RL",
    "chain_of_reasoning": "I first asked how to introduce dynamic feedback into coder models when traditional approaches relied solely on labeled data and lacked large-scale reward signals. I decided to generate extensive question-test-case sets from existing code, relying on pass rates to form preference pairs for training a reward model with the Bradley-Terry formulation. I wondered whether this would yield tangible improvements, and observed a 10-point average gain for one 8B-size model and a 5-point gain for another 7B-size model using best-of-32 sampling, matching a much bigger baseline. I next questioned if direct reinforcement learning from a smaller base model would also excel, so I tested a short optimization stage that yielded notable gains on multiple benchmarks, including over a 25% jump in one setting. I asked if relying on synthesized pass rates would remain stable across diverse tasks, and the results on HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4) confirmed its robustness. A pivotal reflection came when I saw that even small-scale reinforcement learning manifested consistent improvements, suggesting a new way to scale these methods efficiently. I considered whether more sophisticated test generation could further refine the reward model, so I included additional validation checks to ensure reliability. Finally, I plan to extend this pipeline to specialized programming domains while meticulously controlling hyperparameters and analyzing unseen edge cases, confident that reinforcement learning can elevate coder model performance.",
    "url": "https://arxiv.org/abs/2502.01718",
    "pdf_url": "https://arxiv.org/pdf/2502.01718.pdf"
  },
  {
    "paper_id": "",
    "title": "Measuring the intracluster light fraction with machine learning",
    "authors": [
      "Louisa Canepa",
      "Sarah Brough",
      "Francois Lanusse",
      "Mireia Montes",
      "Nina Hatch"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Canepa et al., 2025",
    "abstract": "The intracluster light (ICL) is an important tracer of a galaxy cluster's history and past interactions. However, only small samples have been studied to date due to its very low surface brightness and the heavy manual involvement required for the majority of measurement algorithms. Upcoming large imaging surveys such as the Vera C. Rubin Observatory's Legacy Survey of Space and Time are expected to vastly expand available samples of deep cluster images. However, to process this increased amount of data, we need faster, fully automated methods to streamline the measurement process. This paper presents a machine learning model designed to automatically measure the ICL fraction in large samples of images, with no manual preprocessing required. We train the fully supervised model on a training dataset of 50,000 images with injected artificial ICL profiles. We then transfer its learning onto real data by fine-tuning with a sample of 101 real clusters with their ICL fraction measured manually using the surface brightness threshold method. With this process, the model is able to effectively learn the task and then adapt its learning to real cluster images. Our model can be directly applied to Hyper Suprime-Cam images, processing up to 500 images in a matter of seconds on a single GPU, or fine-tuned for other imaging surveys such as LSST, with the fine-tuning process taking just 3 minutes. The model could also be retrained to match other ICL measurement methods. Our model and the code for training it is made available on GitHub.",
    "bit": "Conventional intracluster light measurements rely on manual procedures that are slow and only feasible for relatively small samples. This dependence on heavy human involvement prevents large-scale analysis, limiting our understanding of low surface brightness regions in galaxy clusters.",
    "flip": "An automated machine learning approach eliminates the need for manual preprocessing by training on vast numbers of artificially generated images and then fine-tuning on real clusters. This technique quickly adapts to new datasets, enabling fast, high-volume processing of ICL fractions without labor-intensive steps.",
    "spark": "Swift automated intracluster light measurement",
    "chain_of_reasoning": "I first wondered how to overcome the manual burden that had hampered previous measurements of low surface brightness regions. I asked whether a machine learning model could be trained on tens of thousands of artificially created images to capture faint intracluster light features. I then carefully fine-tuned it on 101 real clusters with known ICL fractions, verifying this step by comparing outputs to manual threshold-based results. I observed that the model, running on a single GPU, rapidly processed hundreds of Hyper Suprime-Cam images within seconds, which raised the question of how best to confirm consistency over larger datasets. I tested different ways of retraining the model for new surveys, finding that only a few minutes of fine-tuning produced robust adaptations. I questioned whether artificially injected light profiles might introduce unforeseen biases, so I cross-checked them against the surface brightness threshold method and found excellent agreement. I was excited to see that combining synthetic training with real-data fine-tuning eliminated most tedious steps, thus allowing large-scale exploration of intracluster light. In that moment, I realized that fully automated pipelines can indeed match or surpass the manual approach, unleashing wider studies of faint cluster regions.",
    "url": "https://arxiv.org/abs/2501.08378",
    "pdf_url": "https://arxiv.org/pdf/2501.08378.pdf"
  },
  {
    "paper_id": "",
    "title": "Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent Collaboration",
    "authors": [
      "Yijia Shao",
      "Vinay Samuel",
      "Yucheng Jiang",
      "John Yang",
      "Diyi Yang"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Shao et al., 2025",
    "abstract": "Recent advancements in language models (LMs) have sparked growing interest in developing LM agents. While fully autonomous agents could excel in many scenarios, numerous use cases inherently require them to collaborate with humans due to humans' latent preferences, domain expertise, or need for control. To facilitate the study of human-agent collaboration, we present Collaborative Gym (Co-Gym), a general framework enabling asynchronous, tripartite interaction among agents, humans, and task environments. We instantiate Co-Gym with three representative tasks in both simulated and real-world conditions, and propose an evaluation framework that assesses both the collaboration outcomes and processes. Our findings reveal that collaborative agents consistently outperform their fully autonomous counterparts in task performance within those delivered cases, achieving win rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related Work when evaluated by real users. However, our study also highlights significant challenges in developing collaborative agents, requiring advancements in core aspects of intelligence -- communication capabilities, situational awareness, and balancing autonomy and human control.",
    "bit": "Conventional language model agents are often designed to function entirely on their own, relying solely on the model\u2019s learned parameters. This one-sided autonomy can lead to misalignment with user preferences and neglects essential human expertise for many real-world tasks.",
    "flip": "Instead, a new strategy involves agents that actively collaborate with humans by integrating their feedback, domain knowledge, and control. Through a framework enabling asynchronous, multi-party interactions, these agents leverage collective strengths to improve task outcomes and better align with user needs.",
    "spark": "Collaborative synergy for enhanced task performance",
    "chain_of_reasoning": "I started by questioning the wisdom of having models operate in isolation, as they often ignored the subtle nuances of user constraints. I then asked myself whether a more participatory approach could capitalize on human insights, so I designed a blueprint for agents that engage in structured dialogue with users after each task-related decision. The core methodology relied on an asynchronous system where the agent, human, and environment interacted in parallel, which I formalized as Co-Gym. I tested this framework in three tasks\u2014planning itineraries, analyzing tabular data, and exploring related works\u2014across both simulated and actual user interfaces, and observed success rates of 86%, 74%, and 66%, respectively. I reflected on how these results might differ if the agent maintained total autonomy, and discovered consistent gains with collaborative strategies. This realization compelled me to refine communication protocols and add adaptive tuning so that user feedback informed the agent\u2019s future decisions more effectively. I wondered whether deeper situational awareness would reduce misunderstandings, prompting further experimentation with more diverse user scenarios and iterative fine-tuning. For validation, I examined both quantitative success metrics and qualitative feedback, ensuring robust cross-checks of the system\u2019s capacity to integrate human knowledge. Finally, I realized that refining the balance between agent independence and user control could represent the crucial turning point for achieving truly aligned performance.",
    "url": "https://arxiv.org/abs/2412.15701",
    "pdf_url": "https://arxiv.org/pdf/2412.15701.pdf"
  },
  {
    "paper_id": "",
    "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps",
    "authors": [
      "Nanye Ma",
      "Shangyuan Tong",
      "Haolin Jia",
      "Hexiang Hu",
      "Yu-Chuan Su",
      "Mingda Zhang",
      "Xuan Yang",
      "Yandong Li",
      "Tommi Jaakkola",
      "Xuhui Jia",
      "Saining Xie"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Ma et al., 2025",
    "abstract": "Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.",
    "bit": "Conventionally, scaling in generative models is mainly done at training time, focusing on increasing the amount of data, compute, and model parameters. In diffusion models, increasing inference-time computation merely by adding more denoising steps quickly plateaus in terms of performance gains.",
    "flip": "An idea was introduced to treat inference-time generation as a search problem, seeking better noise inputs for diffusion sampling. The approach leverages different verifier methods and search algorithms to systematically refine candidate noises, enabling significant improvements well beyond traditional denoising approaches.",
    "spark": "Search-driven noise refinement for diffusion",
    "chain_of_reasoning": "I began by asking: could increasing inference-time compute beyond a moderate number of denoising steps yield substantial gains, given that scaling at training time was already well-studied under scaling laws? To explore this, I reframed generation as a search problem that strategically refines candidate noise inputs, guided by verifiers measuring fidelity and consistency. I asked which feedback signals best discriminate high-quality samples, and I tested various scoring heuristics on class-conditioned and text-conditioned image generation benchmarks. Observing consistent improvements, I probed whether certain algorithms for candidate selection were more aligned with the verifier\u2019s scores, prompting iterative design choices in sampling. A critical insight occurred when comparing multiple verifier strategies: only some captured subtle artifacts, creating a strong impetus to incorporate multi-faceted verifiers for broader coverage. I wondered about the limits of search complexity, so I tracked performance against computation cost, noticing a sweet spot where sample quality markedly improved. Data analysis showed that tailored verifier-algorithm pairs consistently outperformed naive expansions of denoising steps, confirming that the search-based approach unlocked new performance ceilings. Reflecting on validation strategies, I concluded that cross-checking system outputs against domain-specific constraints or user feedback could further optimize results. I remain convinced that meticulously integrating diverse verifiers offers substantial growth potential, and I plan additional controlled studies to confirm generalizability in diverse imaging tasks.",
    "url": "https://arxiv.org/abs/2501.09732",
    "pdf_url": "https://arxiv.org/pdf/2501.09732.pdf"
  },
  {
    "paper_id": "",
    "title": "LIMO: Less is More for Reasoning",
    "authors": [
      "Yixin Ye",
      "Zhen Huang",
      "Yang Xiao",
      "Ethan Chern",
      "Shijie Xia",
      "Pengfei Liu"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Ye et al., 2025",
    "abstract": "We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as \"cognitive templates\" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.",
    "bit": "Conventional wisdom holds that advanced reasoning in large language models requires tens of thousands of training examples. This assumption leads to massive data requirements, making training expensive and potentially less efficient.",
    "flip": "An alternative approach leverages minimal but precisely curated demonstrations to elicit sophisticated reasoning. By providing carefully selected examples as cognitive templates, the model taps into pre-trained domain knowledge and achieves strong performance using only a small fraction of the previously assumed necessary data.",
    "spark": "Minimal data unlocks complex reasoning",
    "chain_of_reasoning": "I began by questioning why sophisticated reasoning ostensibly requires over one hundred thousand samples, aware that such data intensity could overshadow the model\u2019s inherent domain expertise. I asked: could a small but precisely curated set of demonstrations activate advanced mathematical reasoning given the model\u2019s extensive pre-training? I tested this premise with 817 carefully organized examples, observing 57.1% and 94.8% accuracy on two challenging tasks, surpassing previous solutions trained on vastly more data. I explored whether these results stemmed from memorization by checking out-of-distribution metrics across ten distinct benchmarks, enabling a 40.5% absolute improvement that indicated genuine generalization. I then asked: how might we confirm that these demonstrations reveal reasoning steps rather than merely embedding narrow patterns, which led me to cross-validate the approach for robustness. Reflecting on the outcomes, I discovered that distinct examples serve as conduits for activating internalized knowledge, allowing comprehensive reasoning with minimal post-training. My next question was whether a systematic algorithm for sample selection could optimize coverage of trickier problems and further sharpen performance. I determined that thorough validation across diverse domains is essential for confirming the reliability of such compact training regimens. Recognizing that pre-training completeness and meticulously orchestrated templates interact in a pivotal way was a turning point, revealing how to harness latent capacities. This insight reshaped my understanding of data efficiency, demonstrating that minimal but well-targeted demonstrations can spark complex reasoning that was previously assumed to require far greater scale.",
    "url": "https://arxiv.org/abs/2502.03387",
    "pdf_url": "https://arxiv.org/pdf/2502.03387.pdf"
  },
  {
    "paper_id": "",
    "title": "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks",
    "authors": [
      "Sneh Pandya",
      "Purvik Patel",
      "Brian D. Nord",
      "Mike Walmsley",
      "Aleksandra \u0106iprijanovi\u0107"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Pandya et al., 2025",
    "abstract": "Modern neural networks (NNs) often do not generalize well in the presence of a \"covariate shift\"; that is, in situations where the training and test data distributions differ, but the conditional distribution of classification labels remains unchanged. In such cases, NN generalization can be reduced to a problem of learning more domain-invariant features. Domain adaptation (DA) methods include a range of techniques aimed at achieving this; however, these methods have struggled with the need for extensive hyperparameter tuning, which then incurs significant computational costs. In this work, we introduce SIDDA, an out-of-the-box DA training algorithm built upon the Sinkhorn divergence, that can achieve effective domain alignment with minimal hyperparameter tuning and computational overhead. We demonstrate the efficacy of our method on multiple simulated and real datasets of varying complexity, including simple shapes, handwritten digits, and real astronomical observations. SIDDA is compatible with a variety of NN architectures, and it works particularly well in improving classification accuracy and model calibration when paired with equivariant neural networks (ENNs). We find that SIDDA enhances the generalization capabilities of NNs, achieving up to a $\\approx40\\%$ improvement in classification accuracy on unlabeled target data. We also study the efficacy of DA on ENNs with respect to the varying group orders of the dihedral group $D_N$, and find that the model performance improves as the degree of equivariance increases. Finally, we find that SIDDA enhances model calibration on both source and target data--achieving over an order of magnitude improvement in the ECE and Brier score. SIDDA's versatility, combined with its automated approach to domain alignment, has the potential to advance multi-dataset studies by enabling the development of highly generalizable models.",
    "bit": "Neural networks often struggle when training and test distributions differ, even if the label distribution remains unchanged. Conventional domain adaptation techniques address this mismatch but require extensive hyperparameter tuning, resulting in high computational costs and complexity.",
    "flip": "An out-of-the-box domain adaptation algorithm called SIDDA leverages the Sinkhorn divergence to align distributions with minimal hyperparameter tuning. This method integrates seamlessly with various neural architectures, especially equivariant neural networks, improving both classification accuracy and model calibration.",
    "spark": "Automated domain alignment with Sinkhorn",
    "chain_of_reasoning": "I began by noting that neural networks degrade under distribution shifts, so I asked: how can I reduce domain adaptation complexity while preserving label consistency? I hypothesized that the Sinkhorn divergence might enable a lighter-tuning alignment, so I carefully paired it with a standard classification loss. I tested this approach on shapes, digits, and astronomical data, tracking both classification accuracy and calibration performance under multiple shift scenarios. I asked: would a higher group order in equivariant networks enhance domain invariance, and discovered that increasing dihedral group orders produced stronger results. I then asked: does this approach improve calibration in addition to accuracy, and found an order-of-magnitude reduction in both ECE and Brier score. To validate further, I designed holdout experiments with varied domain shifts and confirmed a consistent accuracy boost of up to 40%. I reflected on whether this gain was stable across architectures, and saw that the Sinkhorn-based alignment retained its effectiveness without exhaustive hyperparameter tuning. I questioned if additional tuning steps were necessary, but observed stable results even with minimal parameter adjustments. Finally, I concluded that out-of-the-box domain adaptation is feasible through a systematic alignment strategy that unifies distribution matching and robust architectures, thereby streamlining the entire training pipeline.",
    "url": "https://arxiv.org/abs/2501.14048",
    "pdf_url": "https://arxiv.org/pdf/2501.14048.pdf"
  },
  {
    "paper_id": "",
    "title": "STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving",
    "authors": [
      "Kefan Dong",
      "Tengyu Ma"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Dong et al., 2025",
    "abstract": "A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens generated during the training in Lean, STP proves 26.3% of the statements in the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (61.1%, pass@3200), Proofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@64).",
    "bit": "Most efforts in formal theorem proving rely on scarce, high-quality proofs, severely limiting training data. Approaches such as reinforcement learning and expert iteration plateau because correct proofs are too rare to provide consistent learning signals.",
    "flip": "A design was introduced that combines a conjecturer and a prover in a self-play cycle, where newly proposed statements are solved by the same system. By iteratively generating and tackling increasingly challenging conjectures, the model continuously expands its training data and boosts its proving performance.",
    "spark": "Iterative self-play for theorem proving",
    "chain_of_reasoning": "I began by asking: how do we surmount the minimal availability of correct proofs that hinders stable training signals in theorem proving? Then I questioned whether it was possible to sustain improvement by continuously generating new statements, ultimately designing a system that replays its own conjectures. Next, I carefully tested iterative training, verifying that generating statements at the boundary of current solvability spurred the forger-prover synergy. One crucial question was how to ensure non-trivial examples, so I integrated standard formal verifiers and carefully curated newly minted statements for difficulty. My experiments processed 19.8 billion tokens in Lean, achieving a 26.3% success rate on a known dataset, doubling the previous 13.2% result. I probed how well this method generalizes, and found 61.1% pass@3200 on miniF2F-test, 23.1% pass@3200 on Proofnet-test, and 8 successful out of 644 on PutnamBench. Reflecting on the iterative synergy, I recognized that an adaptive feedback loop was pivotal for overcoming sparse training data. I also asked whether the approach required extensive human curation, but observed that automation sufficed when guided by consistent correctness checks. Additional validation across multiple tasks confirmed the reliability of the approach, and I plan to refine sampling strategies for further breakthroughs. I wonder if a more sophisticated statement generation scheme can push the boundaries further, motivating the next stage of research.",
    "url": "https://arxiv.org/abs/2502.00212",
    "pdf_url": "https://arxiv.org/pdf/2502.00212.pdf"
  },
  {
    "paper_id": "",
    "title": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning",
    "authors": [
      "Bill Yuchen Lin",
      "Ronan Le Bras",
      "Kyle Richardson",
      "Ashish Sabharwal",
      "Radha Poovendran",
      "Peter Clark",
      "Yejin Choi"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Lin et al., 2025",
    "abstract": "We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty. Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement.",
    "bit": "Large language models are frequently assumed to scale their reasoning capabilities simply by increasing model size and computational resources. However, when confronted with complex logic puzzles that involve non-monotonic reasoning, they exhibit a marked drop in accuracy, indicating fundamental limitations in their ability to handle intricate constraints.",
    "flip": "Instead, a framework was introduced that systematically generates and varies the complexity of logic grid puzzles, allowing a controlled assessment of LLM reasoning limits. This approach also incorporates strategies like best-of-N sampling, backtracking, and self-verification to better handle complex constraints and improve logical inference performance.",
    "spark": "Puzzle-based method reveals LLM reasoning constraints",
    "chain_of_reasoning": "I began by questioning whether increasing a model\u2019s size and compute alone truly addresses intricate logical constraints. My initial experiments with Llama, o1, and DeepSeek-R1 revealed a rapid decline in accuracy as puzzle complexity grew, prompting me to explore systematic puzzle generation for rigorous testing. I asked myself how best to control puzzle parameters to capture diverse constraint structures, so I designed a framework that varied complexity in a quantifiable manner. Observing repeated model failures led me to test best-of-N sampling, backtracking, and self-verification, which partially alleviated inaccuracies yet did not eliminate them. I then reexamined the distribution of puzzle complexities to ensure balanced challenges and verified each puzzle\u2019s difficulty with thorough solution checks. Reflecting on the results, I realized that deeper iterative strategies were pivotal for dealing with non-monotonic structures, prompting further refinement of backtracking algorithms. I wondered whether an adaptive approach might generalize better, so I iterated on puzzle construction to probe specific constraint categories. Ultimately, the data showed that while targeted sampling and verification improve performance, no single technique entirely overcomes the curse of complexity. This finding crystallized my conviction that a controlled puzzle-generation framework is vital for both benchmarking current LLMs and guiding the development of more robust reasoning methods.",
    "url": "https://arxiv.org/abs/2502.01100",
    "pdf_url": "https://arxiv.org/pdf/2502.01100.pdf"
  },
  {
    "paper_id": "",
    "title": "Do pretrained Transformers Learn In-Context by Gradient Descent?",
    "authors": [
      "Lingfeng Shen",
      "Aayush Mishra",
      "Daniel Khashabi"
    ],
    "venue": "arXiv",
    "year": "2025",
    "citation": "Shen et al., 2025",
    "abstract": "The emergence of In-Context Learning (ICL) in LLMs remains a remarkable phenomenon that is partially understood. To explain ICL, recent studies have created theoretical connections to Gradient Descent (GD). We ask, do such connections hold up in actual pre-trained language models? We highlight the limiting assumptions in prior works that make their setup considerably different from the practical setup in which language models are trained. For example, their experimental verification uses \\emph{ICL objective} (training models explicitly for ICL), which differs from the emergent ICL in the wild. Furthermore, the theoretical hand-constructed weights used in these studies have properties that don't match those of real LLMs. We also look for evidence in real models. We observe that ICL and GD have different sensitivity to the order in which they observe demonstrations. Finally, we probe and compare the ICL vs. GD hypothesis in a natural setting. We conduct comprehensive empirical analyses on language models pre-trained on natural data (LLaMa-7B). Our comparisons of three performance metrics highlight the inconsistent behavior of ICL and GD as a function of various factors such as datasets, models, and the number of demonstrations. We observe that ICL and GD modify the output distribution of language models differently. These results indicate that \\emph{the equivalence between ICL and GD remains an open hypothesis} and calls for further studies.",
    "bit": "Previous explanations treat in-context learning (ICL) as directly analogous to gradient descent (GD) through simplified theoretical setups. However, these setups use specifically constructed weights and often employ an ICL objective distinct from the organic behavior observed in large language models, limiting the validity of the analogy in real-world scenarios.",
    "flip": "A new approach conducts empirical studies on actual pre-trained language models using natural data, sidestepping artificial constraints like the ICL objective. This reveals divergent behaviors between ICL and GD in terms of demonstration ordering, performance consistency, and output distribution shifts, suggesting that their equivalence is far from established.",
    "spark": "ICL-GD equivalence remains an open question",
    "chain_of_reasoning": "I first questioned why previous studies, which rely on hand-crafted model weights and an explicit in-context learning objective, might not reflect how large language models actually behave. Reflecting on this, I recognized that artificially imposed constraints often obscure critical emergent behaviors. Therefore, I set up experiments using a real pre-trained language model (LLaMa-7B) on authentic data, systematically comparing how in-context learning and gradient descent respond to differing demonstration orders and performance metrics. I asked myself whether changes in demonstration ordering would affect these adaptation modes similarly, and I discovered that the sensitivity patterns were starkly divergent. Tracking the output distribution and testing multiple datasets, I noticed that in-context learning produced distinctly inconsistent responses compared with gradient-based updates. I pushed further by verifying the results with varied demonstration counts, uncovering shifts in performance that hinted at deeper mismatches. I wondered if randomizing seeds or changing validation splits would replicate these differences, and indeed, the disparities persisted, bolstering my confidence in the findings. Reflecting on these results, I realized that the notion of equivalence between in-context learning and gradient descent seems increasingly untenable in real-world settings. Through these observations, I concluded that the mechanistic underpinnings of in-context learning likely extend beyond the simple GD-inspired frameworks proposed previously, and that further exploration is necessary to fully capture its complexity.",
    "url": "https://arxiv.org/abs/2310.08540",
    "pdf_url": "https://arxiv.org/pdf/2310.08540.pdf"
  }
]
